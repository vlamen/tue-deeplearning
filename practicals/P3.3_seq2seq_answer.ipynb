{"cells":[{"cell_type":"markdown","metadata":{"id":"JIDgHyy2k3YW"},"source":["# P3.3 - Sequence to Sequence: Text Translation\n","\n","In this practical we will develop a model for translation of sentences from German to English using the sequence to sequence architecture.\n","\n","### Learning outcomes\n","- Understand the basic concepts of a sequence to sequence (seq2seq) model\n","- How to preprocess textual data.\n","- How to train a seq2seq model for parametrization of the joint probability distribution $P(y_0, ..., y_k | x_0, ..., x_n)$ over the words $Y$ in the target language, conditioned on the words $X$ of the source sentence.\n","- How to develop a model for translation of sentences from $P(y_0, ..., y_k | x_0, ..., x_n)$.\n","\n","**References**\n","* [1] *Ilya Sutskever, Oriol Vinyals, Quoc V. Le, \"Sequence to Sequence Learning with Neural Networks\"*, NIPS, 2014. https://arxiv.org/abs/1409.3215"]},{"cell_type":"markdown","metadata":{"id":"wNj3zpzuWpVO"},"source":["First, some packages to install in Colab. We need to install some old packages for compatibility.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lmWM6Y3_lls5"},"outputs":[],"source":["!pip install pytorch-nlp\n","!python -m spacy download de\n","!python -m spacy download en\n","!pip install torch==2.3.0 torchtext"]},{"cell_type":"markdown","metadata":{"id":"h1ClfmFsk3YX"},"source":["### Download data\n","\n","We train a translation model on the multi30K dataset. The dataset was specifically designed for machine translation and evaluation tasks and contains (paired) sentences in English/German.\n","\n","We will be processing the dataset via the `torchnlp` library. The dataset is then processed in a similar way as in P3.1_rnn_classification. Since the original multi30k download in `torchnlp` is no longer availabe we provide an alternative source."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RniTBQOjntH1"},"outputs":[],"source":["## boilerplate code to let us download/extract tar.gz downloads ##\n","import json\n","from tqdm.notebook import tqdm\n","import gdown\n","\n","\n","def download_and_extract(url, file_name):\n","    gdown.download(url, file_name, quiet=False)\n","    with open(file_name, 'r', encoding='utf-8') as f:\n","        data = [json.loads(line.strip()) for line in f]\n","    return data\n","\n","\n","train_data = download_and_extract(\"https://drive.google.com/uc?id=1GqE08tMg-dQBbVRiQZ-7eiEHdjl0LXzr\", \"train.jsonl\")\n","valid_data = download_and_extract(\"https://drive.google.com/uc?id=1PIPpx3rm0eYuJw3cJxYpgDlzeIrzeDr9\", \"test.jsonl\")\n","\n","print(f\"Number of training sentences: {len(train_data)}\")\n","print(f\"Number of validation sentences: {len(valid_data)}\\n\\n\")\n","\n","valid_iterator = iter(valid_data)\n","for _ in range(3):\n","    batch = next(valid_iterator)\n","    print(\"DE: \" + batch['de'])\n","    print(\"EN: \" + batch['en'] + '\\n')\n"]},{"cell_type":"markdown","metadata":{"id":"j_sJFs2dk3YZ"},"source":["# Preprocessing textual input data\n","\n","### Create vocabulary\n","As we have seen in practicals/tutorials P1.2 and P3.2, word embeddings are useful for encoding words into vectors of real numbers. The first step is to build a custom vocabulary from the raw training dataset. To this end, we tokenize each sentence and thereafter count the number of occurances of each token (=word or punctuation mark) in each of the articles using `counter`. Finally, we create the vocabulary by using the frequencies of each token in the counter.\n","\n","Note that each datapoint consists of a German and English sentence, thus we create seperate tokenizers and vocabularies for both languages. Futhermore, we add special tokens to both vocabularies: $<unk>$ for unknown tokens, $<pad>$ for padding, $<start>$ and $<end>$ as the first and last tokens of each sentence, respectively."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1f72OvNDk3Ya"},"outputs":[],"source":["import torch\n","from collections import Counter\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import vocab\n","import spacy\n","from tqdm.notebook import tqdm\n","\n","# check if gpu is available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","de_counter, en_counter = Counter(), Counter()\n","\n","de_tokenizer = get_tokenizer('spacy', language='de')\n","en_tokenizer = get_tokenizer('spacy', language='en')\n","\n","for batch in tqdm(train_data):\n","\n","    en, de = batch.values()\n","\n","    de_counter.update(de_tokenizer(de))\n","    en_counter.update(en_tokenizer(en))\n","\n","\n","de_vocab = vocab(de_counter, min_freq=2, specials=['<unk>', '<start>', '<stop>', '<pad>'])\n","en_vocab = vocab(en_counter, min_freq=2, specials=['<unk>', '<start>', '<stop>', '<pad>'])\n","\n","print(f\"Unique tokens in source (de) vocabulary: {len(de_vocab)}\")\n","print(f\"Unique tokens in target (en) vocabulary: {len(en_vocab)}\")"]},{"cell_type":"markdown","metadata":{"id":"Pz0-oBFKk3Yb"},"source":["### Create pipelines\n","\n","In the paper we are implementing, they find it beneficial to reverse the order of the input (they believe it \"introduces many short term dependencies in the data that make the optimization problem much easier\"). We adopt this approach and reverse the German sentence after it has been transformed into a list of tokens."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JC3_0DnNk3Yc"},"outputs":[],"source":["def de_pipeline(text):\n","    \"\"\"\n","    Reverses German sentence and tokizes from a string into a list of strings (tokens). Then converts each token\n","    to corresponding indices. Furthermore, it adds a start token at the appropriate position.\n","    \"\"\"\n","    word_idcs = [de_vocab['<start>']]  # start with start token\n","    vocab_map = de_vocab.get_stoi()  # get our vocab -> idx map\n","\n","    # now, append all words if they exist in the vocab; if not, enter <unk>\n","    # note that we do this in reverse\n","    [word_idcs.append(vocab_map[token] if token in vocab_map else vocab_map['<unk>']) for token in de_tokenizer(text)[::-1]]\n","\n","    word_idcs.append(de_vocab['<stop>'])  # end with stop token\n","\n","    return word_idcs\n","\n","def en_pipeline(text):\n","    \"\"\"\n","    Tokenizes English sentence from a string into a list of strings (tokens), then converts each token\n","    to corresponding indices. Furthermore, it adds a start token at the appropriate position\n","    \"\"\"\n","    word_idcs = [en_vocab['<start>']]  # start with start token\n","    vocab_map = en_vocab.get_stoi()  # get our vocab -> idx map\n","\n","    # now, append all words if they exist in the vocab; if not, enter <unk>\n","    [word_idcs.append(vocab_map[token] if token in vocab_map else vocab_map['<unk>']) for token in en_tokenizer(text)]\n","    word_idcs.append(de_vocab['<stop>'])  # end with stop token\n","    return word_idcs\n","\n"]},{"cell_type":"markdown","metadata":{"id":"w0PmWctDk3Yd"},"source":["The pipelines allow us to convert a string sentence into integers:\n","\n","    en_pipeline('Here is an example!')\n","    >>> [1, 3071, 35, 205, 0, 2176, 2]"]},{"cell_type":"markdown","metadata":{"id":"SfEbt42xXy86"},"source":["For performance reasons, we tokenize all data up front (rather than doing it per-batch) - in this way, we can directly grab the tokenized sentences while training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vzHgP_meJdtf"},"outputs":[],"source":["# tokenize all data up front\n","train_tokenized = [(torch.tensor(en_pipeline(sentence['en']), dtype=torch.int64, device=device),\n","                    torch.tensor(de_pipeline(sentence['de']), dtype=torch.int64, device=device))\n","                    for sentence in tqdm(train_data)]\n","valid_tokenized = [(torch.tensor(en_pipeline(sentence['en']), dtype=torch.int64, device=device),\n","                   torch.tensor(de_pipeline(sentence['de']), dtype=torch.int64, device=device))\n","                   for sentence in tqdm(valid_data)]"]},{"cell_type":"markdown","metadata":{"id":"cbS9LdXZk3Yd"},"source":["### Create DataLoaders\n","Using the tokenized data we create a `collate_batch` method that produces batches of source and target sentences. The `collate_batch` will be used in the `DataLoader` which enables iterating over the dataset in batches. In each iteration, a batch of source sentences (German) and target sentences (English) should be returned. We use the tokenized data and pad all sequences to create two tensors: one containing the input sentences, and another one for the target sentences."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_jE2953Sk3Yd"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","from torch.nn.utils.rnn import pad_sequence\n","\n","def collate_batch(batch):\n","    \"\"\"\n","    Concatenate multiple datapoints to obtain a single batch of data\n","    \"\"\"\n","    # sentences are stored as tuples; get respective lists\n","    en_list = [x[0] for x in batch]\n","    de_list = [x[1] for x in batch]\n","\n","    # pad sequences in batch\n","    de_padded = pad_sequence(sequences = de_list,\n","                             batch_first = True,\n","                             padding_value = de_vocab['<pad>'])\n","    en_padded = pad_sequence(sequences = en_list,\n","                             batch_first = True,\n","                             padding_value = en_vocab['<pad>'])\n","\n","    # return source (DE) and target sequences (EN) after transferring them to GPU (if available)\n","    return de_padded.to(device).T, en_padded.to(device).T\n","\n"]},{"cell_type":"markdown","metadata":{"id":"zxy6nmm-k3Ye"},"source":["# Building the Seq2Seq translation model\n","\n","In the implementation we define three objects: the encoder, the decoder and a full translation model that encapsulates the encoder and decoder. The given code also proposes the main hyperparameters that your implementation should use. Feel free to change the values of these parameters!\n","\n","The referenced paper uses a 4-layer LSTM, but in the interest of training time we can reduce this to 2-layers."]},{"cell_type":"markdown","metadata":{"id":"wXsOvLqTk3Ye"},"source":["## Encoder\n","\n","The encoder takes as input a (batch of) German sentence(s). We already converted all sentences into a zero-padded 2D matrix (shape batch_size, max_seq_len)) containing the tokens that make up the sequences. As output, we want the hidden state to be used as initial state for the Decoder model.\n","\n","**Exercise**:\n","Complete the Encoder's class. In the `__init__(self)` you should declare the approriate layers, where `forward(self, src)` declares the forward pass. The encoder should return a compact representation of the input sequence."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q0C9Uqnfk3Ye"},"outputs":[],"source":["import torch.nn as nn\n","\n","class Encoder(nn.Module):\n","    def __init__(self, source_vocab, emb_dim, hid_dim, n_layers, dropout):\n","        super().__init__()\n","\n","\n","        self.source_vocab = source_vocab\n","        self.hid_dim = hid_dim\n","        self.n_layers = n_layers\n","\n","        ### Your code here ###\n","        self.embedding = nn.Embedding(len(source_vocab), emb_dim)\n","\n","        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","\n","    def forward(self, src):\n","        \"\"\"\n","        Forward pass of encoder model. It aims at\n","        transforming the input sentence to a dense vector\n","\n","        Input:\n","        src shape:  [max_seq_len_in_batch, batch_size]\n","\n","        Output:\n","        hidden and cell dense vectors (hidden and cell)\n","        which contains all sentence information, shape [n layers, batch size, hid dim]\n","        \"\"\"\n","\n","        ### Your code here ###\n","        #src = [src len, batch size]\n","\n","        embedded = self.dropout(self.embedding(src))\n","        #embedded = [src len, batch size, emb dim]\n","\n","        _, (hidden, cell) = self.rnn(embedded)\n","\n","        return hidden, cell"]},{"cell_type":"markdown","metadata":{"id":"O5hZFjsgk3Yf"},"source":["### Decoder"]},{"cell_type":"markdown","metadata":{"id":"DTx4m1Ovk3Yf"},"source":["**Exercise**\n","\n","The next step is to implement the decoder. The Decoder class aims at performing a single step of decoding, i.e. it outputs a single token per time-step. In the first decoding step, the decoder's hidden state is initialized with that of the encoder, and it takes as input the start token (<<l>start>). Then, it should update the cell and hidden state and predict the first real word (i.e. $s_2$ -- not the <<l>start> token) of the target sentence.\n","\n","Generally, in a decoder step, we receive the hidden state from the previous time-step, $(h_{t-1}, c_{t-1})$, and update it with the current embedded token, $y_t$, as input (i.e the embedding that of the token predicted at the end of the previous step). We produce a new hidden state, $(h_t, c_t)$, and predict the next token in the sentence."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JqcX9GTxk3Yf"},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self, target_vocab, emb_dim, hid_dim, n_layers, dropout):\n","        super().__init__()\n","\n","        self.target_vocab = target_vocab\n","        self.hid_dim = hid_dim\n","        self.n_layers = n_layers\n","\n","        ### Your code here ###\n","        self.embedding = nn.Embedding(len(target_vocab), emb_dim)\n","\n","        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n","\n","        self.fc_out = nn.Linear(hid_dim, len(target_vocab))\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","\n","    def forward(self, input, hidden, cell):\n","        \"\"\"\n","        Forward pass of the decoder model. It aims at transforming\n","        the dense representation of the encoder into a sentence in\n","        the target language\n","\n","        Input:\n","        hidden shape: [n layers, batch size, hid dim]\n","        cell shape: [n layers, batch size, hid dim]\n","        input shape: [batch size]  # 1 token for each sentence in the batch\n","\n","        Output:\n","        prediction shape: [batch size, num_words_target_vocabulary]\n","        hidden shape: [n layers, batch size, hid dim]\n","        cell shape: [n layers, batch size, hid dim]\n","        \"\"\"\n","\n","        ### Your code here ###\n","        # pytorch expects a sequence, but we use batches with 1 element, i.e., sequence length 1\n","        input = input.unsqueeze(0)\n","        #input = [1, batch size]\n","\n","        embedded = self.dropout(self.embedding(input))\n","        #embedded = [1, batch size, emb dim]\n","\n","        #\n","        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n","        #output = [1, batch size, hid dim]\n","\n","        prediction = self.fc_out(output.squeeze(0))  # squeeze our 'sequence length 1' away\n","        #prediction = [batch size, output dim]\n","\n","        return prediction, hidden, cell"]},{"cell_type":"markdown","metadata":{"id":"Tp10TDuSk3Yf"},"source":["## The seq2seq model"]},{"cell_type":"markdown","metadata":{"id":"gIN1RPULk3Yf"},"source":["**Exercise**\n","\n","The Seq2Seq model takes in an Encoder, Decoder, and a device (used to place tensors on the GPU, if it exists).\n","For this implementation, we you have to ensure that the number of layers and the hidden (and cell) dimensions are equal in the Encoder and Decoder.\n","\n","Start with declaring the optimizer and loss function of the model. The loss function should not penalize if the ground truth token is the <<l>pad> token. Use the `ignore_index` input argument of the loss function to realize this behavior.\n","\n","\n","The forward method takes the source sentence, target sentence and a teacher-forcing ratio. The teacher forcing ratio is used when training our model. When decoding, at each time-step the decoder will predict what the next token in the target sequence will be from the previous tokens decoded, $\\hat{y}_{t+1}=f(s_t)$. With probability equal to the teaching forcing ratio (`teacher_forcing_ratio`) we will use the actual ground-truth next token in the sequence as the input to the decoder during the next time-step. However, with probability (1 - `teacher_forcing_ratio`), your model should use the token that the LSTM predicted at the end of the previous step, even if it doesn't match the actual next token in the sequence. The `random.random()` will be useful here.\n","\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9hRtG3IQk3Yg"},"outputs":[],"source":["import torch.optim as optim\n","import random\n","\n","class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder, device):\n","        super().__init__()\n","\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.device = device\n","\n","        ### Your code here ###\n","        self.optimizer = optim.Adam(self.parameters())\n","\n","        TRG_PAD_IDX = en_vocab.get_stoi()['<pad>']\n","        self.criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n","\n","\n","    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n","        \"\"\"\n","        Forward pass of the seq2seq model. It encodes the source sentence into\n","        a dense representation and thereafter transduces into the target\n","        sentence.\n","\n","        Inputs:\n","        src: padded index representation of source sentences with shape [src len, batch size]\n","        trg:  padded index representation of target sentences with shape [trg len, batch size]\n","        teacher_forcing_ratio: probability to use teacher forcing, e.g. 0.5 we use ground-truth target sentence 50% of the time\n","\n","        Outputs:\n","        outputs: padded index representation of the predicted sentences with shape [trg_len, batch_size, trg_vocab_size]\n","        \"\"\"\n","\n","        #src = [src len, batch size]\n","        #trg = [trg len, batch size]\n","        #teacher_forcing_ratio is probability to use teacher forcing\n","        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n","\n","        batch_size = trg.shape[1]\n","        trg_len = trg.shape[0]\n","        trg_vocab_size = len(self.decoder.target_vocab)\n","\n","        ### Your code here ###\n","        #tensor to store decoder outputs\n","        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n","\n","        #last hidden state of the encoder is used as the initial hidden state of the decoder\n","        hidden, cell = self.encoder(src)\n","\n","        #first input to the decoder is the <sos> tokens\n","        input = trg[0]\n","        for t in range(1, trg_len):\n","            #insert input token embedding, previous hidden and previous cell states\n","            #receive output tensor (predictions) and new hidden and cell states\n","            output, hidden, cell = self.decoder(input, hidden, cell)\n","\n","            #place predictions in a tensor holding predictions for each token\n","            outputs[t] = output\n","\n","            #decide if we are going to use teacher forcing or not\n","            teacher_force = random.random() < teacher_forcing_ratio\n","\n","            #get the highest predicted token from our predictions\n","            top1 = output.argmax(1)\n","\n","            #if teacher forcing, use actual next token as next input\n","            #if not, use predicted token\n","            input = trg[t] if teacher_force else top1\n","        return outputs"]},{"cell_type":"markdown","metadata":{"id":"gu8xcKQ-k3Yg"},"source":["## Training"]},{"cell_type":"markdown","metadata":{"id":"iYFpNsL1k3Yg"},"source":["**Exercise**\n","\n","Write functions for training and evaluating your model. You should iterate over the dataset and update the weights of the networks with the computed loss value. Print the value of training and validation loss at the end of each epoch.\n","\n","Next, you will need to call your `seq2seq` model and train it using the functions that you implemented. Finally, make a plot of the training and validation accuracy.\n","        \n","Don't forget to declare `best_valid_loss` at the top of the cell, e.g. with\n","\n","    best_valid_loss = float('inf')\n","    \n","To avoid exploding gradients, you can also use gradient clipping, e.g. with\n","\n","    torch.nn.utils.clip_grad_norm_(seq2seq.parameters(), clip)\n","\n","    \n","Finally, the GPU memory will gradually increase which eventually triggers a memory error. Make sure to clear the GPU memory before running the forward pass using the `torch.cuda.empty_cache()` command."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yliz8AtKk3Yh"},"outputs":[],"source":["BATCH_SIZE = 128\n","N_EPOCHS = 15\n","CLIP = 1\n","DROPOUT = 0.5\n","N_LAYERS = 2 #paper uses 4\n","\n","EMB_DIM = 256  #dimension of the word embedding\n","HIDDEN_DIM = 512 #dimension of the lstm's hidden state"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J3X1ukKuk3Yh"},"outputs":[],"source":["# initiate seq2seq translation model\n","enc = Encoder(de_vocab, EMB_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT)\n","dec = Decoder(en_vocab, EMB_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT)\n","\n","seq2seq = Seq2Seq(enc, dec, device).to(device)\n","\n","def init_weights(m):\n","    for name, param in m.named_parameters():\n","        nn.init.uniform_(param.data, -0.08, 0.08)\n","\n","seq2seq.apply(init_weights)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FKrRtcjSk3Yh"},"outputs":[],"source":["import time\n","\n","def train(dataset, clip=CLIP):\n","    epoch_loss = 0\n","    seq2seq.train()\n","    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, collate_fn=collate_batch)\n","\n","    ### Your code here ###\n","    for i, (src, trg) in enumerate(tqdm(dataloader)):\n","\n","        seq2seq.optimizer.zero_grad()\n","\n","        output = seq2seq(src, trg)\n","\n","        #trg = [trg len, batch size]\n","        #output = [trg len, batch size, output dim]\n","\n","        output_dim = output.shape[-1]\n","\n","        output = output[1:].reshape(-1, output_dim)\n","        trg = trg[1:].reshape(-1)\n","\n","        #trg = [(trg len - 1) * batch size]\n","        #output = [(trg len - 1) * batch size, output dim]\n","\n","        loss = seq2seq.criterion(output, trg)\n","\n","        loss.backward()\n","\n","        torch.nn.utils.clip_grad_norm_(seq2seq.parameters(), clip)\n","\n","        seq2seq.optimizer.step()\n","\n","        epoch_loss += loss.item()\n","\n","        torch.cuda.empty_cache()\n","\n","    return epoch_loss / len(dataloader)\n","\n","\n","def evaluate(dataset):\n","    epoch_loss = 0\n","    seq2seq.eval()\n","    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, collate_fn=collate_batch)\n","\n","    ### Your code here ###\n","    with torch.no_grad():\n","\n","        for i, (src, trg) in enumerate(dataloader):\n","\n","            output = seq2seq(src, trg, 0) #turn off teacher forcing\n","\n","            #trg = [trg len, batch size]\n","            #output = [trg len, batch size, output dim]\n","\n","            output_dim = output.shape[-1]\n","\n","            output = output[1:].reshape(-1, output_dim)\n","            trg = trg[1:].reshape(-1)\n","\n","            #trg = [(trg len - 1) * batch size]\n","            #output = [(trg len - 1) * batch size, output dim]\n","\n","            loss = seq2seq.criterion(output, trg)\n","\n","            epoch_loss += loss.item()\n","\n","            torch.cuda.empty_cache()\n","\n","    return epoch_loss / len(dataloader)\n","\n","\n","best_valid_loss = float('inf')\n","train_loss_arr = []; val_loss_arr = []\n","for epoch in range(N_EPOCHS):\n","\n","    epoch_start_time = time.time()\n","\n","    train_loss = train(train_tokenized)\n","    val_loss = evaluate(valid_tokenized)\n","\n","    train_loss_arr.append(train_loss); val_loss_arr.append(val_loss)\n","\n","    if val_loss < best_valid_loss:\n","        best_val_loss = val_loss\n","        torch.save(seq2seq.state_dict(), 'p3_3-model.pt')\n","\n","    print('-' * 76)\n","    print('| end of epoch {:3d} | time: {:5.2f}s | '\n","          'train loss {:8.3f} '\n","          'valid loss {:8.3f} '.format(epoch,\n","                                           time.time() - epoch_start_time,\n","                                           train_loss_arr[-1],\n","                                           val_loss_arr[-1]))\n","    print('-' * 76)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sBKNe651k3Yi"},"outputs":[],"source":["### Plot the training/validation accuracy vs. epochs ###\n","%pylab inline\n","import matplotlib.pyplot as plt\n","\n","f, ax = plt.subplots(1, 1, figsize=(10,8))\n","\n","fnt=16\n","ax.plot(train_loss_arr, color='blue', label='Train')\n","ax.plot(val_loss_arr, color='red', linestyle='--', label='Val')\n","ax.legend(fontsize=fnt)\n","ax.tick_params(axis='both', labelsize=fnt)\n","\n","ax.set_xlabel(\"Epoch\", fontsize=fnt)\n","ax.set_ylabel(\"Accuracy\", fontsize=fnt)"]},{"cell_type":"markdown","metadata":{"id":"2Lu2vv9Xk3Yi"},"source":["# Inference\n","\n","The trained model parametrizes the joint probability distribution $P(Y|X)$ of an English target sentence $Y$ that is a correct translation of the German source sentence $X$. Formally, we seek the sentence $Y$ which maximizes $P(Y|X)$, i.e.\n","\n","$$\n","Y = \\underset{Y^{'}}{\\operatorname{argmax}} p(Y^{â€²}|X).\n","$$\n","\n","**Exercise**\n","\n","During inference using the seq2seq model you can make certain assumptions that should affect your implementation choices. You can assume conditional independence of the targets $P(Y|X)=P(y_{0:k}|X)=P(y_0|X)P(y_1|X)...P(y_k|X)$. In this case you can implement a greedy decoder that computes the most likely output at each step without taking into acount the selected outputs at previous steps. Or you can implement an autoregressive decoder that computes the joint probability of the output given the input $P(Y|X)=P(y_{0:k}|X)=P(y_0|X)P(y_1|y_0, X)...P(y_k|y_{0:k-1},X)$.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"njvX68MTk3Yi"},"outputs":[],"source":["### Your code here ###\n","def idx_to_sen(sentence_idcs, vocab):\n","    sentence_idcs = sentence_idcs[sentence_idcs > 3] #remove special tokens\n","    sentence_idcs = np.array(vocab.get_itos())[sentence_idcs]\n","    return ' '.join(sentence_idcs)\n","\n","def print_val_examples(src, trg, pred, N):\n","    for src_, trg_, pred_ in zip(src[:N], trg[:N], pred[:N]):\n","        print(f' src: {src_}\\n trg: {trg_}\\n pred: {pred_}\\n')\n","\n","def greedy_decoder(dataset):\n","\n","    seq2seq.eval()\n","\n","    epoch_loss = 0\n","\n","    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, collate_fn=collate_batch)\n","    predf = []; srcf = []; trgf = []\n","\n","    with torch.no_grad():\n","\n","        for i, (src, trg) in enumerate(dataloader):\n","            output = seq2seq(src, trg, 0) # no teacher forcing during inference\n","            top1 = output.argmax(2)\n","\n","            for p, s, t in zip(top1.T.cpu(), src.T.cpu(), trg.T.cpu()):\n","                predf.append(idx_to_sen(p, en_vocab))\n","                srcf.append(idx_to_sen(torch.flip(s, (0, )), de_vocab))\n","                trgf.append(idx_to_sen(t, en_vocab))\n","\n","        return srcf, trgf, predf\n","\n","out = greedy_decoder(valid_tokenized)\n","\n","print_val_examples(*out, N=10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LyKz3Adgu437"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.4"}},"nbformat":4,"nbformat_minor":0}