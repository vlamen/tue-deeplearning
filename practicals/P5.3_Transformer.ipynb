{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vlamen/tue-deeplearning/blob/main/practicals/P5.3_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#P5.3 - Text Translation (using Transformers)"
      ],
      "metadata": {
        "id": "L37Va8RHX6-4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this practical we will implement a Transformer [1] from scratch and apply it to the task of text translation (same as in P3.3). We will use the PyTorch library."
      ],
      "metadata": {
        "id": "PtE-Z7LlYJ0c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning outcomes\n",
        "\n"
      ],
      "metadata": {
        "id": "dOepaZ4CYjc4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Understand the basic concept of positional embeddings in transformers\n",
        "*   Understand and implement the basic concept and underlying mechanism of multi-head attention\n",
        "*   Learn how to train a transformer for the parametrization of the joint probability distribution $P(y_0,...,y_k|x_0,...,x_n)$ over the words $Y$ in the targe language, conditioned on the words $X$ of the source sequence"
      ],
      "metadata": {
        "id": "h1XQuGjpt3-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation\n",
        "We will follow roughly the same steps as P3.3 to prepare the text data. For more information on what each function does, you can revisit P3.3."
      ],
      "metadata": {
        "id": "W4j3r2gps-Y8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Downloading"
      ],
      "metadata": {
        "id": "6__UAfgjuQlb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_FDIS7JRn1O"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch-nlp\n",
        "!python -m spacy download de\n",
        "!python -m spacy download en\n",
        "!pip install torch==2.3.0 torchtext"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## boilerplate code to let us download/extract tar.gz downloads ##\n",
        "import json\n",
        "from tqdm.notebook import tqdm\n",
        "import gdown\n",
        "\n",
        "\n",
        "def download_and_extract(url, file_name):\n",
        "    gdown.download(url, file_name, quiet=False)\n",
        "    with open(file_name, 'r', encoding='utf-8') as f:\n",
        "        data = [json.loads(line.strip()) for line in f]\n",
        "    return data\n",
        "\n",
        "\n",
        "train_data = download_and_extract(\"https://drive.google.com/uc?id=1GqE08tMg-dQBbVRiQZ-7eiEHdjl0LXzr\", \"train.jsonl\")\n",
        "valid_data = download_and_extract(\"https://drive.google.com/uc?id=1PIPpx3rm0eYuJw3cJxYpgDlzeIrzeDr9\", \"test.jsonl\")\n",
        "\n",
        "print(f\"Number of training sentences: {len(train_data)}\")\n",
        "print(f\"Number of validation sentences: {len(valid_data)}\\n\\n\")\n",
        "\n",
        "valid_iterator = iter(valid_data)\n",
        "for _ in range(3):\n",
        "    batch = next(valid_iterator)\n",
        "    print(\"DE: \" + batch['de'])\n",
        "    print(\"EN: \" + batch['en'] + '\\n')\n"
      ],
      "metadata": {
        "id": "phXz9d1_tM6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data preprocessing"
      ],
      "metadata": {
        "id": "Z6TnQsJCt6zC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from collections import Counter\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import vocab\n",
        "import spacy\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# check if gpu is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "de_counter, en_counter = Counter(), Counter()\n",
        "\n",
        "de_tokenizer = get_tokenizer('spacy', language='de')\n",
        "en_tokenizer = get_tokenizer('spacy', language='en')\n",
        "\n",
        "for batch in tqdm(train_data):\n",
        "\n",
        "    en, de = batch.values()\n",
        "\n",
        "    de_counter.update(de_tokenizer(de))\n",
        "    en_counter.update(en_tokenizer(en))\n",
        "\n",
        "\n",
        "de_vocab = vocab(de_counter, min_freq=2, specials=['<unk>', '<start>', '<stop>', '<pad>'])\n",
        "en_vocab = vocab(en_counter, min_freq=2, specials=['<unk>', '<start>', '<stop>', '<pad>'])\n",
        "\n",
        "print(f\"Unique tokens in source (de) vocabulary: {len(de_vocab)}\")\n",
        "print(f\"Unique tokens in target (en) vocabulary: {len(en_vocab)}\")"
      ],
      "metadata": {
        "id": "hgjU7JUmtPxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipeline creation"
      ],
      "metadata": {
        "id": "cvaGxMW6t9BG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def de_pipeline(text):\n",
        "    \"\"\"\n",
        "    Reverses German sentence and tokizes from a string into a list of strings (tokens). Then converts each token\n",
        "    to corresponding indices. Furthermore, it adds a start token at the appropriate position.\n",
        "    \"\"\"\n",
        "    word_idcs = [de_vocab['<start>']]  # start with start token\n",
        "    vocab_map = de_vocab.get_stoi()  # get our vocab -> idx map\n",
        "\n",
        "    # now, append all words if they exist in the vocab; if not, enter <unk>\n",
        "    # note that we do this in reverse\n",
        "    [word_idcs.append(vocab_map[token] if token in vocab_map else vocab_map['<unk>']) for token in de_tokenizer(text)[::-1]]\n",
        "\n",
        "    word_idcs.append(de_vocab['<stop>'])  # end with stop token\n",
        "\n",
        "    return word_idcs\n",
        "\n",
        "def en_pipeline(text):\n",
        "    \"\"\"\n",
        "    Tokenizes English sentence from a string into a list of strings (tokens), then converts each token\n",
        "    to corresponding indices. Furthermore, it adds a start token at the appropriate position\n",
        "    \"\"\"\n",
        "    word_idcs = [en_vocab['<start>']]  # start with start token\n",
        "    vocab_map = en_vocab.get_stoi()  # get our vocab -> idx map\n",
        "\n",
        "    # now, append all words if they exist in the vocab; if not, enter <unk>\n",
        "    [word_idcs.append(vocab_map[token] if token in vocab_map else vocab_map['<unk>']) for token in en_tokenizer(text)]\n",
        "    word_idcs.append(de_vocab['<stop>'])  # end with stop token\n",
        "    return word_idcs"
      ],
      "metadata": {
        "id": "2LhZl1wquK1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize all data up front\n",
        "train_tokenized = [(torch.tensor(en_pipeline(sentence['en']), dtype=torch.int64, device=device),\n",
        "                    torch.tensor(de_pipeline(sentence['de']), dtype=torch.int64, device=device))\n",
        "                    for sentence in tqdm(train_data)]\n",
        "valid_tokenized = [(torch.tensor(en_pipeline(sentence['en']), dtype=torch.int64, device=device),\n",
        "                   torch.tensor(de_pipeline(sentence['de']), dtype=torch.int64, device=device))\n",
        "                   for sentence in tqdm(valid_data)]"
      ],
      "metadata": {
        "id": "HWKH1_I0uYMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer implementation"
      ],
      "metadata": {
        "id": "sq4pYpNB2wXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will implement the different components of the transformer architecture (see below). The given code also proposes the main hyperparameters your code should use. Feel free to change the values of these parameters!\n",
        "\n",
        "<center><img src=\"https://raw.githubusercontent.com/vlamen/tue-deeplearning/main/img/transformer.png\" alt=\\\"None\\\" width=\\\"500\\\"/></center>"
      ],
      "metadata": {
        "id": "XJ7m1oTQ3AM0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Positional encoding"
      ],
      "metadata": {
        "id": "jyZF2zwN6mWa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first transformer component we will be implementing is the positional encoding. Since transformers do not make use of the order of the sequence, information about the order must be injected manually in the model. This is done through positional encodings. To generate the positional encoding of a sequence, the following formulas are used:\n",
        "\n",
        "$PE_{pos,2i} = sin(pos/10000^{2i/d_{model}})$\n",
        "\n",
        "$PE_{pos,2i+1} = cos(pos/10000^{2i/d_{model}})$\n",
        "\n",
        "In these formulas, $pos$ is the position, while $i$ is the index of the embedding dimension ($d_{model}$). For the uneven indices of the embedding dimension, the cosine is used, while for the even dimension the sine is used. The embeddings of the words and their position both have the same size, such that they can be summed together. After the summation, a dropout layer is applied.\n",
        "\n",
        "In the ```PositionalEncoding``` class below, implement the calculation of the positional encoding discussed above. You can use the ```pe``` variable for the positional encoding. The ```position``` variable already contains all possible positions."
      ],
      "metadata": {
        "id": "olWNOmJ_6qcm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_length, dropout):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pe = torch.zeros(max_seq_length, d_model)\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "        ### YOUR CODE HERE ###\n",
        "\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # sum word embedding and positional encoding\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "\n",
        "        ### YOUR CODE HERE ###\n",
        "\n",
        "        return"
      ],
      "metadata": {
        "id": "9Tr-gjvX6cIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mulite-Head attention"
      ],
      "metadata": {
        "id": "OH3F8bI6Iu0p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will implement the (multi-head) attention mechanism of transformers. Recall that in transformers, attention is calculated using the queries ($Q$), keys ($K$) and values ($V$). These are obtained using a linear transformation of the current embeddings. The formula for a single attention head is as follows, where the $d_k$ (embedding size) is the scaling factor:\n",
        "\n",
        "$ \\text{Attention}( Q, K, V) = \\text{softmax}\\left( \\frac{QK^T}{\\sqrt{d_k}}V \\right) $\n",
        "\n",
        "A drawback of using a single attention head is that only one type of attention can be calculated. To mitigate this issue, multiple attention heads can be used, where each attention head uses a different $Q_i$, $K_i$ and $V_i$. The output of the different attention heads is then concatenated at the end to obtain a new embedding for each word.\n",
        "\n",
        "In practice, we can generate queries, keys and values by first performing the linear transformations on the input embeddings. Then, we can split the output into the different $Q_i$, $K_i$ and $V_i$.\n",
        "\n",
        "\n",
        "Below, complete the ```MultiHeadAttention``` class. In the class initialization ```d_model``` indicates the hidden state of the input/output embeddings (512 in the paper), while ```num_heads``` indicates the amount of attention heads used (8 in the paper).\n",
        "\n",
        " Note that in the transformer a mask is used. In the encoder, it is used to ensure that attention is only calculated between embeddings of words (and not padding tokens). In the decoder, it is used to not to restrict the transformer from accessing \"future\" tokens. To apply the mask in the ```scaled_dot_product_attention``` function, you can consider the following code, where we fill the masked values with a large negative number:\n",
        "\n",
        "```attn_scores = attn_scores.masked_fill(mask == 0, -1e9)```\n",
        "\n",
        "**Hint:** When performing the splitting and combining of the heads, keep in mind what the data shape should be. The input (and output) of the ```MultiHeadAttention``` module will have shape ```[batch size, seq_length, d_model]```.\n"
      ],
      "metadata": {
        "id": "vrUmUDpZ7lda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.d_model = d_model # input embedding size\n",
        "        self.num_heads = num_heads # number of attention heads\n",
        "        self.d_k = d_model // num_heads # attention embedding size\n",
        "\n",
        "        ### YOUR CODE HERE ###\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "\n",
        "        ### YOUR CODE HERE ###\n",
        "\n",
        "        return output\n",
        "\n",
        "    def split_heads(self, x):\n",
        "\n",
        "        ### YOUR CODE HERE ###\n",
        "\n",
        "        return x\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "\n",
        "        ### YOUR CODE HERE ###\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "\n",
        "        ### YOUR CODE HERE ###\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "dc4wWOeqEgsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Position-wise Feed-Forward Networks"
      ],
      "metadata": {
        "id": "PmYPzatZM21r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will implement the position-wise feed-forward networks used by the transformers. These are performed by applying a two layer MLP to each embedding, with an input and output size of ```d_model```, and a hidden state of ```d_ff``` and a ReLU activation between the two layers. The network can be described using the following formula:\n",
        "\n",
        "$\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2$\n",
        "\n",
        "In the original transformer paper, the input and output size has a dimensionality of 512, while the hidden state has a size of 2048."
      ],
      "metadata": {
        "id": "37V0sMUzNMOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))"
      ],
      "metadata": {
        "id": "M-RP40ZzNLYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder Block"
      ],
      "metadata": {
        "id": "fx9P7xdxPQOM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With all the required components defined, lets bring it all together to create a single encoding block, as shown in the picture below:\n",
        "\n",
        "<center><img src=\"https://raw.githubusercontent.com/vlamen/tue-deeplearning/main/img/transformer_encoder_block.png\" alt=\\\"None\\\" width=\\\"500\\\"/></center>\n",
        "\n",
        "In a single block, first, a Multi-Head Attention layer is applied, with dropout used on the output. After that, the residual connection is applied, which is followed by normalization. For the next part of the block, the same steps are applied, with the attention layer being changed to the the Point-wise Feed-Forward network.\n",
        "\n",
        "**Note** In the transformer, layer normalization is used. In PyTorch, this is implemented in the ```nn.LayerNorm``` module."
      ],
      "metadata": {
        "id": "KLLkXMzgVUhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(EncoderBlock, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "        return x"
      ],
      "metadata": {
        "id": "wBs03H92VTH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder\n",
        "\n"
      ],
      "metadata": {
        "id": "9x1c5_EMYBB6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create the full encoder, we will stack multiple encoder blocks together. In the original paper, 6 blocks were used. Both in the positional encoding and each block, a dropout probability of 0.1 was used.\n",
        "\n"
      ],
      "metadata": {
        "id": "FPbhchGtYJJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length, dropout)\n",
        "        self.layers = nn.ModuleList([EncoderBlock(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x = self.embedding(x)\n",
        "        x = self.positional_encoding(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "lzQI1Is9YHSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder Block"
      ],
      "metadata": {
        "id": "gLMS1Bnh_djQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, we will need to create a decoder block. Compared to the encoder block, the decoder block contains two multi-head attention layers. The first layer is masked, such that tokens can only attend to \"past\" tokens, and not look into the future. The second attention layer calculates attention between the embeddings of the decoder, and the output embeddings from the encoder.\n",
        "\n",
        "<center><img src=\"https://raw.githubusercontent.com/vlamen/tue-deeplearning/main/img/transformer_decoder_block.png\" alt=\\\"None\\\" width=\\\"500\\\"/></center>"
      ],
      "metadata": {
        "id": "KFKwbl_1_fp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, out_enc, enc_mask, dec_mask):\n",
        "        attn_output = self.self_attn(x, x, x, dec_mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        attn_output = self.cross_attn(x, out_enc, out_enc, enc_mask)\n",
        "        x = self.norm2(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm3(x + self.dropout(ff_output))\n",
        "        return x"
      ],
      "metadata": {
        "id": "73YTpUwN_SYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder"
      ],
      "metadata": {
        "id": "KYFh0L4ABJ4I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will stack multiple blocks together to form the full decoder. In the original paper, 6 blocks were stacked on top of each other. Following the decoder blocks, there is a final layer to map the output embeddings to predictions."
      ],
      "metadata": {
        "id": "s8QcQ3mCBMtW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length, dropout)\n",
        "\n",
        "        self.layers = nn.ModuleList([DecoderBlock(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "\n",
        "        self.pred_layer = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x, out_enc, enc_mask, dec_mask):\n",
        "        x = self.embedding(x)\n",
        "        x = self.positional_encoding(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, out_enc, enc_mask, dec_mask)\n",
        "\n",
        "        out = self.pred_layer(x)\n",
        "        return out"
      ],
      "metadata": {
        "id": "dmJiSxIOBLZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer"
      ],
      "metadata": {
        "id": "oqdIcqFiWdWu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we have all the required parts, we can build the full transformer. We will also add a function to create 2 masks (which we have already seen previously). The first mask, ```enc_mask```, will mask out padding tokens in the input sentence, since we do not want to attend to these. The second mask, ```dec_mask```, masks the padding tokens in the output sentence. However, in addition to masking the padding tokens, it also masks \"future\" decoder tokens, such that the decoder cannot make use of future information during training.\n",
        "\n",
        "Thanks to the mask used in the decoder, we only need to perform one forward pass through the model (during trainign), whereas in a RNN model this would neep to happen autoregressively.\n",
        "\n"
      ],
      "metadata": {
        "id": "zuC_Ux8ZWgI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, enc_vocab_size, dec_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(enc_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
        "        self.decoder = Decoder(dec_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
        "\n",
        "    def forward(self, enc_in, dec_in, enc_mask, dec_mask):\n",
        "        out_enc = self.encoder(enc_in, enc_mask)\n",
        "        out = self.decoder(dec_in, out_enc, enc_mask, dec_mask)\n",
        "        return out"
      ],
      "metadata": {
        "id": "W7XwIKEfWfZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Dataloaders"
      ],
      "metadata": {
        "id": "ueozgIntu4kz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def decoder_mask(size):\n",
        "    # Creating a square matrix of dimensions 'size x size' filled with ones\n",
        "    mask = torch.triu(torch.ones(1, size, size), diagonal = 1).type(torch.int)\n",
        "    return mask == 0\n",
        "\n",
        "\n",
        "def collate_batch(batch):\n",
        "    \"\"\"\n",
        "    Concatenate multiple datapoints to obtain a single batch of data\n",
        "    \"\"\"\n",
        "    # sentences are stored as tuples; get respective lists\n",
        "    en_list = [x[0] for x in batch]\n",
        "    de_list = [x[1] for x in batch]\n",
        "\n",
        "    # pad sequences in batch\n",
        "    de_padded = pad_sequence(sequences = de_list,\n",
        "                             batch_first = True,\n",
        "                             padding_value = de_vocab['<pad>'])\n",
        "    en_padded = pad_sequence(sequences = en_list,\n",
        "                             batch_first = True,\n",
        "                             padding_value = en_vocab['<pad>'])\n",
        "\n",
        "    # decoder input, remove last token\n",
        "    en_padded_in = en_padded[:,:-1]\n",
        "\n",
        "    # decoder output (target), remove first token\n",
        "    en_padded_out = en_padded[:, 1:]\n",
        "\n",
        "    de_mask = (de_padded != de_vocab['<pad>']).unsqueeze(1).unsqueeze(1).int()\n",
        "\n",
        "    # mask is calculated for the decoder input\n",
        "    en_mask_in = (en_padded_in != en_vocab['<pad>']).unsqueeze(1).unsqueeze(1).int()\n",
        "    dec_mask = decoder_mask(en_padded_in.size(-1)).unsqueeze(0).to(device)\n",
        "\n",
        "    # combine the two masks\n",
        "    en_mask = en_mask_in & dec_mask\n",
        "\n",
        "    # return source (DE) and target sequences (EN) after transferring them to GPU (if available)\n",
        "    return de_padded.to(device), en_padded_in.to(device), en_padded_out.to(device), de_mask.to(device), en_mask.to(device)"
      ],
      "metadata": {
        "id": "zW4Qp9jiubmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training"
      ],
      "metadata": {
        "id": "zgje69As71NV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With everything in place, lets train a transformer! Below, we provide some hyperparameters, but feel free to change this to your own preferences."
      ],
      "metadata": {
        "id": "knIzXdG9Ugio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQ_LENGTH = max([len(i[0]) for i in train_tokenized] + \\\n",
        "                     [len(i[1]) for i in train_tokenized] + \\\n",
        "                     [len(i[0]) for i in valid_tokenized] + \\\n",
        "                     [len(i[1]) for i in valid_tokenized]) + 1"
      ],
      "metadata": {
        "id": "TKJzCOYk9ODI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "EPOCHS = 15\n",
        "LR = 0.001\n",
        "\n",
        "DROPOUT = 0.1\n",
        "N_HEADS = 4\n",
        "N_LAYERS = 4\n",
        "HIDDEN_DIM = 64\n",
        "FF_HIDDEN_DIM = 64"
      ],
      "metadata": {
        "id": "T4fbJEVv8u02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = Transformer(enc_vocab_size = len(de_vocab),\n",
        "                          dec_vocab_size = len(en_vocab),\n",
        "                          d_model = HIDDEN_DIM,\n",
        "                          num_heads = N_HEADS,\n",
        "                          num_layers = N_LAYERS,\n",
        "                          d_ff = FF_HIDDEN_DIM,\n",
        "                          max_seq_length = MAX_SEQ_LENGTH,\n",
        "                          dropout=DROPOUT,\n",
        "                          ).to(device)"
      ],
      "metadata": {
        "id": "Qc3Ajruz72rL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "\n",
        "optimizer = optim.Adam(transformer.parameters(), lr=LR)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=en_vocab['<pad>'])"
      ],
      "metadata": {
        "id": "JAX2cBgSEQYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainloader = DataLoader(train_tokenized, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "validloader = DataLoader(valid_tokenized, batch_size=BATCH_SIZE, collate_fn=collate_batch)"
      ],
      "metadata": {
        "id": "u2Q98hqkJJrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, we implement the training loop, as well as helper functions for training and evaluating for a full epoch. Since the transformer does not need to perform autoregressive forward passes during training, these functions remain relatively simple."
      ],
      "metadata": {
        "id": "f29EdXHqVfyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataloader, optimizer):\n",
        "\n",
        "    epoch_loss = 0.0\n",
        "    model.train()\n",
        "\n",
        "    for i, (src, tgt_in, tgt_out, src_mask, tgt_mask) in enumerate(tqdm(dataloader)):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        tgt_hat = model(src, tgt_in, src_mask, tgt_mask).transpose(-1,-2)\n",
        "\n",
        "        loss = criterion(tgt_hat, tgt_out)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss/len(dataloader)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, dataloader):\n",
        "\n",
        "    epoch_loss = 0.0\n",
        "    model.eval()\n",
        "\n",
        "    for i, (src, tgt_in, tgt_out, src_mask, tgt_mask) in enumerate(tqdm(dataloader)):\n",
        "        tgt_hat = model(src, tgt_in, src_mask, tgt_mask).transpose(-1,-2)\n",
        "        loss = criterion(tgt_hat, tgt_out)\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss/len(dataloader)"
      ],
      "metadata": {
        "id": "a1e3HQf-JJEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Rkt20JzRoCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "SAVE=False\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "train_loss_arr = []; val_loss_arr = []\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    train_loss = train(transformer, trainloader, optimizer)\n",
        "    val_loss = evaluate(transformer, validloader)\n",
        "\n",
        "    train_loss_arr.append(train_loss); val_loss_arr.append(val_loss)\n",
        "\n",
        "    if SAVE and (val_loss < best_valid_loss):\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(transformer.state_dict(), 'p5_3-model.pt')\n",
        "\n",
        "    print('-' * 76)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
        "          'train loss {:8.3f} '\n",
        "          'valid loss {:8.3f} '.format(epoch,\n",
        "                                           time.time() - epoch_start_time,\n",
        "                                           train_loss_arr[-1],\n",
        "                                           val_loss_arr[-1]))\n",
        "    print('-' * 76)"
      ],
      "metadata": {
        "id": "wQqcDbkzM9QX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model evaluation"
      ],
      "metadata": {
        "id": "PpXNkc08V6eF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've trained the model, let's see how it performs! When running inference, the transformer is not able to produce a translated sentence in a single forward pass. Like the RNN seq2seq models, this decoding happens autoregressively, producing one new token at a time. Below, the greedy decoder is implemented, where we simply add the next most likely word to the sentence at each autoregressive step."
      ],
      "metadata": {
        "id": "asUeLMr0V8p8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "### Your code here ###\n",
        "def idx_to_sen(sentence_idcs, vocab):\n",
        "    sentence_idcs = sentence_idcs[sentence_idcs > 3] #remove special tokens\n",
        "    sentence_idcs = np.array(vocab.get_itos())[sentence_idcs]\n",
        "    return ' '.join(sentence_idcs)\n",
        "\n",
        "def print_val_examples(src, trg, pred, N):\n",
        "    for src_, trg_, pred_ in zip(src[:N], trg[:N], pred[:N]):\n",
        "        print(f' src: {src_}\\n trg: {trg_}\\n pred: {pred_}\\n')\n",
        "\n",
        "@torch.no_grad()\n",
        "def greedy_decoder(model, dataloader):\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    predf = []; srcf = []; trgf = []\n",
        "\n",
        "    for idx, (src, tgt_in, tgt_out, src_mask, tgt_mask) in tqdm(enumerate(dataloader)):\n",
        "\n",
        "        out_enc = model.encoder(src, src_mask)\n",
        "\n",
        "        sentence = tgt_in[:,[0]]\n",
        "\n",
        "        sen_len = 1\n",
        "        while True:\n",
        "\n",
        "            dec_mask = decoder_mask(sentence.size(1)).unsqueeze(0).to(device)\n",
        "            out = model.decoder.forward(sentence, out_enc, src_mask, dec_mask)\n",
        "\n",
        "            _, next_word = torch.max(out[:,-1], dim=1, keepdim=True)\n",
        "            if sen_len == MAX_SEQ_LENGTH:\n",
        "                break\n",
        "\n",
        "            sentence = torch.cat([sentence, next_word], dim=1)\n",
        "            sen_len += 1\n",
        "\n",
        "        for p, s, t in zip(sentence.cpu(), src.cpu(), tgt_out.cpu()):\n",
        "            predf.append(idx_to_sen(p, en_vocab))\n",
        "            srcf.append(idx_to_sen(s, de_vocab))\n",
        "            trgf.append(idx_to_sen(t, en_vocab))\n",
        "\n",
        "\n",
        "\n",
        "    return srcf, trgf, predf\n",
        "\n",
        "out = greedy_decoder(transformer, validloader)\n",
        "\n",
        "print_val_examples(*out, N=10)\n"
      ],
      "metadata": {
        "id": "Al1bmtuCNMLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tdfDt2E2BlX1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}