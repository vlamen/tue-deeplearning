{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "JiwpncEfEQ6w",
   "metadata": {
    "id": "JiwpncEfEQ6w"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/vlamen/tue-deeplearning/blob/main/practicals/P4.2_Graph_Classification_Solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XZBHIfDJveSN",
   "metadata": {
    "id": "XZBHIfDJveSN"
   },
   "source": [
    "# P4.2: Graph Classification (using Graph Neural Networks)\n",
    "In this practical we will apply Graph Neural Networks (GNNs) to the task of\n",
    "classifying entire graphs. We will use PyTorch and the PyTorch Geometric library\n",
    "[[1]](#pytorchgeomintro).\n",
    "\n",
    "Upon completing this practical, you will have:\n",
    "\n",
    "*   Studied a hands-on introduction to the graph classification problem;\n",
    "*   Enhanced your understanding of Graph Neural Networks by implementing a\n",
    "    message passing GNN (as described in the lecture notes) from scratch in\n",
    "    PyTorch;\n",
    "*   Learned how to use the PyTorch Geometric library for minibatching graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9oo1kU_Qjb",
   "metadata": {
    "id": "cc9oo1kU_Qjb"
   },
   "source": [
    "# Preparation\n",
    "To start, we install the packages we need, set up some formatting for the\n",
    "notebook and download the data we use in this practical. If necessary, uncomment\n",
    "and run the cell below to install the required packages (for example if you're\n",
    "using google colab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "djyRhMjVChaz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "djyRhMjVChaz",
    "outputId": "c4438934-8cf0-4521-865e-e5f459f720ed"
   },
   "outputs": [],
   "source": [
    "# PyTorch Geometric is not part of the standard Colab packages; we must\n",
    "# install it ourselves.\n",
    "# !pip install torch_geometric\n",
    "\n",
    "# PyTorch Geometric version 2.5.2 should work with PyTorch version\n",
    "# 2.2.1. If the Colab environment by now has a later version of PyTorch\n",
    "# installed and if that does not yet work with PyTorch Geometric, you\n",
    "# can install specific versions of torch and torch_geometric by using\n",
    "# the commands below instead of the one above. This might take a few\n",
    "# minutes.\n",
    "# !pip install torch==2.2.1+cu121 torchvision==0.17.1+cu121 torchaudio==2.2.1+cu121 --extra-index-url https://download.pytorch.org/whl/cu121\n",
    "# !pip install torch_geometric==2.5.2\n",
    "\n",
    "# Optional dependencies:\n",
    "# !pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.2.1+cu121.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TmqpM9AyDGkj",
   "metadata": {
    "id": "TmqpM9AyDGkj"
   },
   "outputs": [],
   "source": [
    "# We want to limit the height of the output cells.\n",
    "from IPython.display import HTML, display, Javascript\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bda20d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "id": "22bda20d",
    "outputId": "0a656e53-5d54-4eca-8a60-b4b22217dbc6"
   },
   "outputs": [],
   "source": [
    "# We will use the Mutagenicity dataset, which is part of the standard\n",
    "# datasets in torch geometric. downloading may take some time\n",
    "from torch_geometric.datasets import TUDataset\n",
    "import numpy as np\n",
    "dataset = TUDataset(root='data/mutagen', name='Mutagenicity', use_node_attr=True, use_edge_attr=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a84ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the below list and dict maps an integer to the corresponiding element\n",
    "# string and an element string to a color respectively. this will be\n",
    "# useful for plotting molecules later on\n",
    "\n",
    "element_map = ['C', 'O', 'Cl', 'H', 'N', 'F', 'Br', 'S', 'P', 'I', 'Na', 'K', 'Li', 'Ca']\n",
    "element_color = {\n",
    "    'H': [1.0, 1.0, 1.0],\n",
    " 'Li': [0.8, 0.5019607843137255, 1.0],\n",
    " 'C': [0.5647058823529412, 0.5647058823529412, 0.5647058823529412],\n",
    " 'N': [0.18823529411764706, 0.3137254901960784, 0.9725490196078431],\n",
    " 'O': [1.0, 0.050980392156862744, 0.050980392156862744],\n",
    " 'F': [0.5647058823529412, 0.8784313725490196, 0.3137254901960784],\n",
    " 'Na': [0.6705882352941176, 0.3607843137254902, 0.9490196078431372],\n",
    " 'P': [1.0, 0.5019607843137255, 0.0],\n",
    " 'S': [1.0, 1.0, 0.18823529411764706],\n",
    " 'Cl': [0.12156862745098039, 0.9411764705882353, 0.12156862745098039],\n",
    " 'K': [0.5607843137254902, 0.25098039215686274, 0.8313725490196079],\n",
    " 'Ca': [0.23921568627450981, 1.0, 0.0],\n",
    "'Br': [0.6509803921568628, 0.1607843137254902, 0.1607843137254902],\n",
    " 'I': [0.5803921568627451, 0.0, 0.5803921568627451]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UUy-_h3cAW_N",
   "metadata": {
    "id": "UUy-_h3cAW_N"
   },
   "source": [
    "# Problem Formulation\n",
    "The type of data we consider is graph data, where a graph is defined by its\n",
    "nodes and edges, $\\mathcal{G} = (\\mathcal{V},\\mathcal{E})$. The dataset consists\n",
    "of many (comparatively small) graphs and we seek to classify these graphs in\n",
    "their entirety. As such, in this practical, a *graph* is analogous to an image\n",
    "in an image classification task (as opposed to the earlier tutorial, where we\n",
    "tried to classify each *node* in a graph).\n",
    "\n",
    "We can define our data $X$ as a set of graphs, i.e., $X = \\{\\mathcal{G}_0,\n",
    "\\mathcal{G}_1, \\ldots, \\mathcal{G}_n\\}$, wich corresponding labels $\\{Y_0, Y_1,\n",
    "\\ldots, Y_n\\}$.\n",
    "\n",
    "## Dataset: Database of molecules and their mutagenicity\n",
    "The dataset we look at for this practical consists of molecules and their\n",
    "mutagenicity property[[2]](#mutagenicity). This property describes whether a\n",
    "molecule can induce genetic mutations or not, making it a binary attribute. \n",
    "\n",
    "The datapoints in the dataset are molecular graphs $\\mathcal{G}_i \\in X$, and\n",
    "each such molecule-graph is assigned a label 0 (no mutagenicity) or 1\n",
    "(mutagenicity). The nodes in the molecule-graph indicate (chemical) elements\n",
    "and edges indicate connections between these elements.\n",
    "\n",
    "We want to train a GNN to take these graphs as input and output a prediction.\n",
    "Note that there are no constraints on these graphs in terms of size or\n",
    "connections, so our model must be able to process arbitrary graphs. This is\n",
    "where the use of GNNs shines, as this class of NN-models can process graphs of\n",
    "arbitrary shape.\n",
    "\n",
    "Next, we will further examine our dataset and split it into a train and\n",
    "validation set. Note that for this practical, we don't use a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_peXi7DIfAbG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "id": "_peXi7DIfAbG",
    "outputId": "95fea4fe-645c-4613-db97-c17fd6c1e0cb"
   },
   "outputs": [],
   "source": [
    "print(f'Number of examples: {len(dataset)}')\n",
    "print('')\n",
    "print(\"Let's print a few examples' shape:\")\n",
    "for i in range(4):\n",
    "    print(dataset[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kS1M89qFfr4s",
   "metadata": {
    "id": "kS1M89qFfr4s"
   },
   "source": [
    "The dataset consists of 4337 molecules. As we can see from printing a few\n",
    "examples, they vary in shape. As a reminder of what these tensors are, for an\n",
    "arbitrary graph $\\mathcal{G}$:\n",
    "\n",
    "$edge\\_attr=[\\mathcal{G}_\\mathcal{E}, \\mathcal{E}_a]$: edge attributes. For each\n",
    "edge $\\mathcal{E}$ in graph $\\mathcal{G}$, we have $\\mathcal{E}_a$ attributes.\n",
    "For simplicity, we do not use these attributes in this notebook, but once you\n",
    "have a GNN architecture set up, it should be straightforward to incorporate edge\n",
    "features as well.\n",
    "\n",
    "$edge\\_index=[2, \\mathcal{G}_\\mathcal{E}]$: the tensor describing all edges in\n",
    "the graph. The first row describes the source node of each edge and the second\n",
    "row describes the target node of each edge. E.g., $edge\\_index=[[0, 1, 2], [3,\n",
    "4, 5]]$ describes edges $(0, 3), (1, 4)$ and $(2, 5)$. For undirected graphs (as\n",
    "in this practical), we assume that the edge index contains both $(i ,j)$ and\n",
    "$(j,i)$ for all connected nodes $i, j$.\n",
    "\n",
    "$x=[\\mathcal{G}_\\mathcal{V}, \\mathcal{V}_a]$: the tensor describing all nodes\n",
    "and their attributes. For each node $\\mathcal{V}$ in graph $\\mathcal{G}$, we\n",
    "have $\\mathcal{V}_a$ attributes.\n",
    "\n",
    "$y=[1]$: The (binary) label for the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CPAl4NJGhzXh",
   "metadata": {
    "id": "CPAl4NJGhzXh"
   },
   "source": [
    "The number of nodes and edges ($\\mathcal{G}_\\mathcal{V}$ and\n",
    "$\\mathcal{G}_\\mathcal{E}$) can differ for each graph, whereas the node and edge\n",
    "attributes ($\\mathcal{V}_a$ and $\\mathcal{E}_a$) are fixed for the entire\n",
    "dataset. As stated before, we do not use the edge attributes in this practical.\n",
    "The node attributes *are* used. Each node has a one-hot vector that indicates\n",
    "what element the node is. The following 14 elements are used in this dataset:\n",
    "```\n",
    "['C', 'O', 'Cl', 'H', 'N', 'F', 'Br', 'S', 'P', 'I', 'Na', 'K', 'Li', 'Ca']\n",
    "```\n",
    "For each node, the attribute is a one-hot encoding with the index of the 1\n",
    "indicating what element it is, according to the list printed above.\n",
    "\n",
    "We aim to use these node attributes ($x$) along with their connections\n",
    "($edge\\_index$) to predict the mutagenicity of the molecule ($y$). To get an\n",
    "idea of what a single datapoint looks like, we define a function to draw one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1964036f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "1964036f",
    "outputId": "223a40e1-9318-446d-d6a7-397d40d8ce42"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_molecule(graph, title=''):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    nodes = {}  # create a node dict (idx:element) to create a nx graph\n",
    "    for i in range(graph.x.shape[0]):\n",
    "        element_idx = np.argmax(graph.x[i])\n",
    "        nodes[i] = element_map[element_idx]\n",
    "    edges = []  # create an edge list for the nx graph\n",
    "    for i in range(graph.edge_index.shape[1]):\n",
    "        s, t = graph.edge_index[:, [i]]\n",
    "        s, t = int(s), int(t)\n",
    "        edges.append((s, t))\n",
    "\n",
    "    g = nx.Graph()  # create a graph\n",
    "    g.add_nodes_from(nodes)\n",
    "    g.add_edges_from(edges) \n",
    "    \n",
    "    pos = nx.planar_layout(g)  # the graph has no 'position': generate a node-layout\n",
    "    pos = nx.spring_layout(g, pos=pos)\n",
    "\n",
    "    colors = [element_color[i] for _, i in nodes.items()]  # set the color for each node\n",
    "    nx.draw(g, pos=pos, labels=nodes, node_color=colors, width=1)  # and draw the graph\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    ax.collections[0].set_edgecolor(\"#000000\")  # color the edges\n",
    "    \n",
    "    display(HTML(\"\"\"<style>#output-body {display: flex;align-items: center;justify-content: center;}</style>\"\"\"))  # center the image\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7acb62",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "dc7acb62",
    "outputId": "6dad8e1a-439f-46f9-8e8a-601f8236b128"
   },
   "outputs": [],
   "source": [
    "idx = 0\n",
    "draw_molecule(dataset[idx], f'Sample {idx}, Mutagenicity = {bool(dataset[idx].y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Wl13S34gkjvE",
   "metadata": {
    "id": "Wl13S34gkjvE"
   },
   "source": [
    "Note that the location / layout of the nodes is somewhat arbitrary, and is\n",
    "generated by our drawing function. The graph information we have only considers\n",
    "the nodes' element, alongside what other nodes they are connected to. There are\n",
    "no 'coordinates' available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kVafXLUnlJQ-",
   "metadata": {
    "id": "kVafXLUnlJQ-"
   },
   "source": [
    "Next, we will split our dataset into a train and validation set and define a\n",
    "dataloader. The latter allows us to process graphs in minibatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p_xlCEMvAAw1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "p_xlCEMvAAw1",
    "outputId": "82b8dd49-57ad-4644-b224-51bf80c527d9"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "We simply shuffle and split the dataset to create a train and validation\n",
    "set.\n",
    "'''\n",
    "dataset.shuffle() # first, shuffle our dataset\n",
    "train_idx = round(len(dataset) * 0.8) # 80:20 for the train:validation split\n",
    "dataset_train = dataset[:train_idx]\n",
    "dataset_val = dataset[train_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97691e3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "d97691e3",
    "outputId": "fb45418e-ce88-449d-9e0c-93722c288a7d"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "To process the dataset in minibatches, we use a DataLoader object. All\n",
    "we must do is supply the data and set the batch_size, PyTorch Geometric\n",
    "will take care of the rest. Note that the batch_size indicates the\n",
    "number of graphs in a batch, not the number of nodes.\n",
    "'''\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset_val, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Zdbbb1sqptX1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Zdbbb1sqptX1",
    "outputId": "25fe4542-2cd5-4610-a3f0-c048cb4e9891"
   },
   "outputs": [],
   "source": [
    "# To understand how graphs are batched, print one\n",
    "for data in train_loader:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "azAAam4rp0IP",
   "metadata": {
    "id": "azAAam4rp0IP"
   },
   "source": [
    "Minibatching with graphs is slightly trickier than with structured inputs, as\n",
    "each graph consists of different amounts of nodes and edges, and PyTorch\n",
    "requires all tensors in the batch to be of the same size. In short, to\n",
    "efficiently process all nodes in a batch, PyTorch Geometric concatenates all\n",
    "node/edge data for all graphs in the batch to a single graph object, where each\n",
    "individual graph is a disconnected component (which explains the singular and\n",
    "large $x$/$edge\\_index$, etc). In computer memory, this graph is represented as\n",
    "a single tensor, allowing more efficient processing on the GPU. To be able to\n",
    "identify to which individual graph a node belongs, a $batch$ tensor maps each\n",
    "node to the graph it belongs to. For more information, we refer to the\n",
    "[documentation](https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html#mini-batches)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QRYLDURxAwJS",
   "metadata": {
    "id": "QRYLDURxAwJS"
   },
   "source": [
    "# Graph Classification\n",
    "Now that we have discussed the problem and dataset, we can move towards a\n",
    "solution to the problem. Although PyTorch Geometric already implements many\n",
    "common GNN architectures, the goal of this practical is to deepen your\n",
    "understanding by implementing a GNN from scratch in PyTorch. This will also help\n",
    "you in possible future projects or assignments where off-the-shelf GNN\n",
    "architectures may be suboptimal.\n",
    "\n",
    "In the below cells, we provide some imports that will come in handy, as well as\n",
    "a few skeleton classes that can help you to structure your implementation.\n",
    "\n",
    "An accuracy of 75-80% on the validation set should be achievable with a\n",
    "relatively simple network and without too much parameter tuning, within 100\n",
    "training epochs. Indicitavely, my own solution takes about 400 seconds of\n",
    "training time on my laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1493c6e5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "1493c6e5",
    "outputId": "e0365ff4-a07e-400b-fda8-01074554e3a7"
   },
   "outputs": [],
   "source": [
    "num_features = int(dataset_train[0].x.shape[1])\n",
    "num_classes = int(max([d.y for d in dataset_train])+1)\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adccad0",
   "metadata": {},
   "source": [
    "$$x_i^{(k)} = \\gamma^{(k)} \\left( x_i^{(k-1)}, \\cup_{j \\in N(i)} \\phi^{(k)} \\left(x_i^{(k-1)}, x_j^{(k-1)}, e_{j,i} \\right) \\right) $$\n",
    "\n",
    "$\\gamma$ is the node update function, $\\cup$ is the aggregation function, and $\\phi$ is the edge transfer function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OWFeY_tDmlnn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "OWFeY_tDmlnn",
    "outputId": "cca5c962-7d76-4f7e-9eb2-0e16d2469e42"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implement a message passing GNN. Hint: you might want to look into\n",
    "torch.Tensor.scatter_add_() or similar functions for aggregating tensors\n",
    "at specified indices.\n",
    "\"\"\"\n",
    "\n",
    "class Node_to_emb(nn.Module):  # transforms input nodes to an embedding (similar to word embedding in NLP)\n",
    "    #### why would an embedding layer be useful?\n",
    "\n",
    "    def __init__(self, node_feat_dim=14, node_emb_dim=64):\n",
    "        super().__init__()\n",
    "        self.emb_dim = node_emb_dim\n",
    "        self.node_dim = node_feat_dim\n",
    "        self.emb = nn.Linear(self.node_dim, self.emb_dim)\n",
    "        \n",
    "\n",
    "    def forward(self, nodes):\n",
    "        assert nodes.size(-1) == self.node_dim, 'wrong input dimension of node features!'\n",
    "        out = self.emb(nodes)\n",
    "        return out\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "class MpLayer(torch.nn.Module):  # a neural message passing layer\n",
    "    def __init__(self, hidden_dim, activation=nn.ReLU()):\n",
    "        super(MpLayer, self).__init__()\n",
    "        \n",
    "        #  Hint: which neural networks are used in neural message passing?\n",
    "        \n",
    "        self.edge_network = nn.Sequential(nn.Linear(2*hidden_dim, hidden_dim),\n",
    "                                          activation,\n",
    "                                          nn.Linear(hidden_dim, hidden_dim),\n",
    "                                          activation\n",
    "                                          )\n",
    "        \n",
    "        self.node_network = nn.Sequential(nn.Linear(2*hidden_dim, hidden_dim),\n",
    "                                          activation,\n",
    "                                          nn.Linear(hidden_dim, hidden_dim),\n",
    "                                          )\n",
    "        \n",
    "    def forward(self, input_to_layer):\n",
    "        node_tensor, edge_idx_tensor = input_to_layer\n",
    "        edge_messages_input = torch.concat([node_tensor[edge_idx_tensor[0,:]], node_tensor[edge_idx_tensor[1,:]]], dim=-1) # shape (num_edges, 2*node_dim)\n",
    "        edge_messages_output = self.edge_network(edge_messages_input) # shape (num_edges, hidden_dim)\n",
    "        \n",
    "        #now aggregate the edge messages for each node the edge points to:\n",
    "        \n",
    "        node_agg_messages = torch.zeros(node_tensor.size(0), node_tensor.size(1)).to(node_tensor.device)\n",
    "        node_agg_messages = node_agg_messages.scatter_add_(\n",
    "            dim=0, index=edge_idx_tensor[1].unsqueeze(-1).expand(-1, node_tensor.size(1)), src=edge_messages_output\n",
    "        )\n",
    "        \n",
    "        #### why does the aggregation function need to be permutationally invariant? What is another aggregation function\n",
    "        #### that could be used?\n",
    "        \n",
    "        #put the aggregated messages through the node update network:\n",
    "        node_out = self.node_network(torch.cat([node_tensor, node_agg_messages], dim=-1))\n",
    "\n",
    "        return node_out, edge_idx_tensor\n",
    "        \n",
    "        \n",
    "\n",
    "class MpGNN(torch.nn.Module): # a message passing GNN\n",
    "    def __init__(self, node_feat_dim, hidden_dim, activation=nn.ReLU(), num_layers=3, num_classes=2):\n",
    "        super(MpGNN, self).__init__()\n",
    "        \n",
    "        #  Hint: the MpGNN must embed the categorical node features, apply message passing layers,\n",
    "        #        and finally predict the mutagenicity of each graph in the batch.\n",
    "        \n",
    "        self.node_to_emb = Node_to_emb(node_feat_dim, hidden_dim)\n",
    "        self.forward_net = nn.Sequential(\n",
    "            *[MpLayer(hidden_dim, activation) for i in range(num_layers)]\n",
    "        )\n",
    "        self.to_pred = nn.Sequential(nn.Linear(hidden_dim, num_classes), torch.nn.Softmax())\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.node_to_emb(x)\n",
    "#         print(x.size(), edge_index.size())\n",
    "        input_model = (x, edge_index)\n",
    "        output_model = self.forward_net(input_model)\n",
    "        x,_ = output_model\n",
    "        \n",
    "        out = torch.zeros(max(batch)+1, x.size(1)).to(x.device)\n",
    "        idx_aggregate_graph = batch.unsqueeze(-1).expand(-1, x.size(1))\n",
    "    \n",
    "\n",
    "        out.scatter_add_(dim=0, index=idx_aggregate_graph, src=x) # aggregate all node embeddings per graph in the batch\n",
    "        \n",
    "        x = self.to_pred(out)\n",
    "        return x\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7adbf6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "ce7adbf6",
    "outputId": "2c9f2a3a-3204-4175-95cf-0102dba5d8b0"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Hint: you might want to try running on the cpu first for easier\n",
    "debugging. Additionally, depending on your implementation, the GPU may\n",
    "not even be faster than the CPU. (note that the focus of this assignment\n",
    "is not the efficiency of the implementation on the GPU).\n",
    "'''\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device='cpu'\n",
    "print(f'Loaded device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65a110c",
   "metadata": {
    "id": "c65a110c"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Training the GNN\n",
    "'''\n",
    "model = MpGNN(14, 32, num_layers=5) # initialize our GNN\n",
    "# print(model)\n",
    "model.to(device)  # and move to the GPU, if possible\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())  # adam usually works well\n",
    "loss_func = torch.nn.CrossEntropyLoss()  # binary classification, so crossentropy is a suitable loss\n",
    "\n",
    "def train():\n",
    "    model.train()  # set the model to training mode\n",
    "    for data in train_loader:  # loop through the training set in a batch-wise fashion\n",
    "        data.to(device)  # move the batch to the device (GPU if applicable)\n",
    "        optimizer.zero_grad()  # set gradients to 0\n",
    "        out = model(data.x, data.edge_index, data.batch)  # propagate the data through the model\n",
    "        y_expanded = torch.zeros(data.y.size(0), 2).to(device)\n",
    "        y_expanded[:, 0] += data.y == 0\n",
    "        y_expanded[:, 1] += data.y == 1\n",
    "#         print(out[:1,:], y_expanded[:1,:])\n",
    "        loss = loss_func(out, y_expanded)  # compute the loss\n",
    "        loss.backward()  # derive gradients\n",
    "        optimizer.step()  # update all parameters based on the gradients\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()  # set the model to evaluation mode (no dropout)\n",
    "    \n",
    "    correct = 0  # keep track of how many we have correct\n",
    "    total = 0  # and how many we handle in total\n",
    "    for data in loader:  # loop through the supplied dataset in a batch-wise fashion\n",
    "        data.to(device)  # transfer batch to device\n",
    "        out = model(data.x, data.edge_index, data.batch)  # propagate the data through the model\n",
    "        pred = out.argmax(dim=1)  # as prediction, we take the class with the highest probability\n",
    "        correct += int((pred == data.y).sum())  # add the number of correct predictions\n",
    "        total += len(data.y)  # and add the total number of elements\n",
    "    return correct / total  # return the accuracy\n",
    "\n",
    "\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "epochs = 100\n",
    "start = time.time()\n",
    "for epoch in range(1, epochs):  # train for 100 epochs\n",
    "    train()  # do one training step over the entire dataset\n",
    "    with torch.no_grad():\n",
    "        train_acc = test(train_loader)  # compute the training accuracy\n",
    "        val_acc = test(val_loader)  # compute the validation accuracy\n",
    "    tic = time.time()\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Validation Acc: {val_acc:.4f}. Training time so far: {tic-start:.1f} s')\n",
    "    train_accs.append(train_acc)  # save accuracies so we can plot them\n",
    "    val_accs.append(val_acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86218833",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "86218833",
    "outputId": "c7bc5a5c-5483-4872-b4ee-0ee61ec4b9de"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "To get an overview of the accuracy over the training period, we define a\n",
    "simple function to plot the saved accuracies.\n",
    "'''\n",
    "def plot_train(train_accs, val_accs):\n",
    "    fig, ax = plt.subplots(figsize=(8,6))\n",
    "    fnt=16\n",
    "    ax.plot(train_accs, color='blue', label='Train')\n",
    "    ax.plot(val_accs, color='red', linestyle='--', label='Validation')\n",
    "    ax.legend(fontsize=fnt)\n",
    "    ax.tick_params(axis='both', labelsize=fnt)\n",
    "    ax.set_xlabel('Epoch', fontsize=fnt)\n",
    "    ax.set_ylabel('Accuracy', fontsize=fnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bx3vOYKJE8if",
   "metadata": {
    "id": "bx3vOYKJE8if"
   },
   "outputs": [],
   "source": [
    "plot_train(train_accs, val_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5cf249",
   "metadata": {
    "id": "bd5cf249"
   },
   "source": [
    "# Evaluation and discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XkI9E--qsrxE",
   "metadata": {
    "id": "XkI9E--qsrxE"
   },
   "source": [
    "To conclude, we shortly evaluate the performance of our model, and use it to\n",
    "make predictions. We define a simple function for testing and drawing a single\n",
    "molecule (from the validation set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ID1fhKtir-",
   "metadata": {
    "id": "75ID1fhKtir-"
   },
   "outputs": [],
   "source": [
    "print(f'Final validation accuracy: {val_accs[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0592b7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "ae0592b7",
    "outputId": "701ace0f-a222-4312-d68b-85992b57dd40"
   },
   "outputs": [],
   "source": [
    "def test_sample(idx):\n",
    "    mol = dataset_val[idx]  # take our sample\n",
    "    mol.to(device)  # put it on device\n",
    "    pred = model(mol.x,  # propagate nodes\n",
    "                 mol.edge_index,   #  edges\n",
    "                 torch.zeros(mol.x.shape[0], dtype=torch.long).to(device))  # 'batch' object of 0s (the one graph in the batch)\n",
    "\n",
    "    pred = pred.to('cpu')  # put data back on cpu\n",
    "    mol = mol.to('cpu')\n",
    "    pred = int(pred.argmax(dim=1))  # gather predictions\n",
    "    true = int(mol.y)  \n",
    "    outcome = ['No mutagenicity', 'Mutagenicity']\n",
    "    outcome_text = 'Predicted = \"{}\", Ground-truth = \"{}\"'.format(outcome[pred], outcome[true])  # create outcome text\n",
    "    draw_molecule(mol, title=f'Sample {idx}\\n' + outcome_text)  # draw the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9365a11",
   "metadata": {
    "id": "b9365a11"
   },
   "outputs": [],
   "source": [
    "test_sample(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZCHMhGnpA1hq",
   "metadata": {
    "id": "ZCHMhGnpA1hq"
   },
   "source": [
    "## Discussion\n",
    "This concludes the practical on graph classification. We have focused on how to\n",
    "use GNNs to classify entire graphs, where the graphs can be of arbitrary size.\n",
    "\n",
    "We credit the Graph Classification tutorial provided by PyTorch\n",
    "Geometric[[3]](#pytorchgeom) as a foundation for this practical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yCaIa48BA2z6",
   "metadata": {
    "id": "yCaIa48BA2z6"
   },
   "source": [
    "# References\n",
    "<a name=\"pytorchgeomintro\"></a>\n",
    "[1] https://pytorch-geometric.readthedocs.io/en/latest/index.html \n",
    "\n",
    "<a name=\"mutagenicity\"></a>\n",
    "[2] Kazius, J., McGuire, R., & Bursi, R. (2005). Derivation and validation of toxicophores for mutagenicity prediction. Journal of medicinal chemistry, 48(1), 312-320.\n",
    "\n",
    "<a name=\"pytorchgeom\"></a>\n",
    "[3] https://pytorch-geometric.readthedocs.io/en/latest/notes/colabs.html"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "P4.2_Graph_Classification.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
