{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dzMfiJReh9G"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vlamen/tue-deeplearning/blob/main/practicals/P2.3_Metric_learning_image_retrieval_answer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "349KbhFJeh9P"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8HYcdfteh9R"
      },
      "source": [
        "# Practical Overview\n",
        "\n",
        " In this practical you will learn :\n",
        "- What is `Cifar 10` dataset and how to download the dataset through PyTorch.\n",
        "- How to build triplet-net from scratch.\n",
        "- How to select informative triplets.\n",
        "- How to perform image retrieval tasks.\n",
        "\n",
        "\n",
        "The practicals include:\n",
        "- Design the embedding net of the triplet model that transforms data to an embeddings space.\n",
        "- Complete the triplet Loss.\n",
        "- Complete the triplets selection strategy.\n",
        "- Perform the image retrieval task and make comparison with results under different selection strategies\n",
        "\n",
        "\n",
        "**Hint**:  \n",
        "\n",
        "The training epoch numbers, learning rate, margin value, optimization method, and function structure/name in this notebook are just for reference, feel free to modify them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcmGBqXeCutw"
      },
      "source": [
        "# Preparing dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbuGweB5eh9T"
      },
      "source": [
        "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
        "\n",
        "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\n",
        "\n",
        "Here are the classes in the dataset, as well as 10 random images from each:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0kHTdTfeh9T"
      },
      "source": [
        "<center><img src=\"https://raw.githubusercontent.com/vlamen/tue-deeplearning/main/img/cifar10.png\" alt=\"Cifar_10\" width=\"500\"/></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojYC6q45eh9U"
      },
      "source": [
        "Before we begin, let us first prepare our data. Luckily enough, we can simply download the dataset through PyTorch and have everything set up for us but you can also download the dataset from here [Cifar 10](https://www.cs.toronto.edu/~kriz/cifar.html):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yobb7MvOeh9V"
      },
      "source": [
        "Before we load our data, we need to first prepare the transformations to be applied to it. This is a necessary step to prepare the data for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7tn-5Fpeh9V"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YM9fOcdMeh9W"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mikEJNKOeh9X"
      },
      "outputs": [],
      "source": [
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                        download=False, transform=transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_h8fJKQeh9a"
      },
      "source": [
        "Let's check the 10 different classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDxpNqppeh9a",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "classes=train_dataset.classes\n",
        "\n",
        "print (classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-W4-qb3neh9b"
      },
      "outputs": [],
      "source": [
        "#Let's visualize some example images\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "for i in range (10):\n",
        "    plt.imshow(train_dataset.data[i])\n",
        "    plt.show()\n",
        "    print (\"Label\",train_dataset.classes[train_dataset.targets[i]])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xtvRE_Meh9c"
      },
      "source": [
        "# Triplet Network\n",
        "We'll train a triplet network, that takes an anchor, positive (same class as anchor) and negative (different class than anchor) examples. The objective is to learn embeddings such that the anchor is closer to the positive example than it is to the negative example by some margin value.\n",
        "\n",
        "\n",
        "<center><img src=\"https://raw.githubusercontent.com/vlamen/tue-deeplearning/main/img/a_n_p.png\" alt=\"Anchor_postive_negative_example\" width=\"500\"/></center>\n",
        "\n",
        "*Schroff, Florian, Dmitry Kalenichenko, and James Philbin. [Facenet: A unified embedding for face recognition and clustering.](https://arxiv.org/abs/1503.03832) CVPR 2015.*\n",
        "\n",
        "**Triplet loss**:   $L_{triplet}(x_a, x_p, x_n) = max(0, m +  \\lVert f(x_a)-f(x_p)\\rVert_2^2 - \\lVert f(x_a)-f(x_n)\\rVert_2^2$\\)$\n",
        "\n",
        "\n",
        "**Hint**:  Please reference notebook chapter 4.5.2 `One-shot/metric learning` for more detail\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iREC7DTfeh9c"
      },
      "source": [
        "\n",
        "## Steps\n",
        "\n",
        "1. Define **embedding** *(mapping)* network $f(x)$ - **EmbeddingNet** , to extract embedding of each sample (anchor, positive and negative image ) in triplets.\n",
        "\n",
        "2. Create **BalancedBatchSampler** - samples $N$ classes and $M$ samples for each class to form a minibatch, and create a dataloader with the batch sampler.\n",
        "\n",
        "3. Define a **TripletSelector** that return triplets combinations by the samples from  **BalancedBatchSampler**\n",
        "\n",
        "4. Define **TripletLoss** that will use a *TripletSelector* and compute *TripletLoss* on  triplets\n",
        "5. Train the network!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiITEdPaeh9d"
      },
      "source": [
        "##  Embedding Net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKySzmy4eh9d"
      },
      "source": [
        " **EmbeddingNet** is the branch of the triplet model that transforms data to an embeddings space. Try to complete the convolution neural network.\n",
        "\n",
        "\n",
        " **Hint**  :\n",
        "\n",
        "(1) You could use `nn.Conv2d`,`nn.BatchNorm2d`,`nn.ReLU`,`nn.MaxPool2d`,`nn.Linear`,from `torch`. Try to choose the proper the `kernel_size` and `in_channels`,`out_channels`,`stride`.\n",
        "    \n",
        "(2) If the model is too shallow, it could collapse to one single point.\n",
        "\n",
        "\n",
        "(3) PyTorch doesn't have a view layer, and we need to create one for our network. Lambda will create a layer that we can then use when defining a network with Sequential.\n",
        "\n",
        "```py\n",
        "class Lambda(nn.Module):\n",
        "    def __init__(self, func):\n",
        "        super().__init__()\n",
        "        self.func = func\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.func(x)\n",
        "```\n",
        "\n",
        "The view layer could be written as:\n",
        "```py\n",
        "Lambda(lambda x: x.view(x.size(0), -1))\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExXkTKpmcijx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Lambda(nn.Module):\n",
        "    def __init__(self, func):\n",
        "        super().__init__()\n",
        "        self.func = func\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.func(x)\n",
        "\n",
        "\n",
        "class EmbeddingNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"CNN Builder.\"\"\"\n",
        "        super(EmbeddingNet, self).__init__()\n",
        "\n",
        "        self.front_layer = nn.Sequential(\n",
        "            # Conv Layer block 1\n",
        "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Conv Layer block 2\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "\n",
        "            # Conv Layer block 3\n",
        "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            Lambda(lambda x: x.view(x.size(0), -1)),\n",
        "\n",
        "            nn.Linear(4096, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.last_layer = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Perform forward.\"\"\"\n",
        "        # conv layers\n",
        "        x = self.front_layer(x)\n",
        "        x = self.last_layer(x)\n",
        "        return x\n",
        "\n",
        "    def get_embedding(self, x):\n",
        "        return self.forward(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka2CPTtteh9e"
      },
      "source": [
        "## Balanced Batch Sampler and Data Loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mEldXZ3eh9e"
      },
      "source": [
        "Let's create `BalancedBatchSampler` to create a minibatch that contains  $N$ classes and $M$ samples for each class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-qQ8Elneh9f"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data.sampler import BatchSampler\n",
        "import numpy as np\n",
        "class BalancedBatchSampler(BatchSampler):\n",
        "    \"\"\"\n",
        "    Returns batches of size n_classes * n_samples\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, labels, n_classes, n_samples):\n",
        "        self.labels = labels\n",
        "        self.labels_set = list(set(self.labels))\n",
        "        self.label_to_indices = {label: np.where(  np.array(self.labels) == label)[0]\n",
        "                                 for label in self.labels_set}\n",
        "        for l in self.labels_set:\n",
        "            np.random.shuffle(self.label_to_indices[l])\n",
        "        self.used_label_indices_count = {label: 0 for label in self.labels_set}\n",
        "        self.count = 0\n",
        "        self.n_classes = n_classes\n",
        "        self.n_samples = n_samples\n",
        "        self.n_dataset = len(self.labels)\n",
        "        self.batch_size = self.n_samples * self.n_classes\n",
        "\n",
        "    def __iter__(self):\n",
        "        self.count = 0\n",
        "        while self.count + self.batch_size < self.n_dataset:\n",
        "            classes = np.random.choice(self.labels_set, self.n_classes, replace=False)\n",
        "            indices = []\n",
        "            for class_ in classes:\n",
        "                indices.extend(self.label_to_indices[class_][\n",
        "                               self.used_label_indices_count[class_]:self.used_label_indices_count[\n",
        "                                                                         class_] + self.n_samples])\n",
        "                self.used_label_indices_count[class_] += self.n_samples\n",
        "                if self.used_label_indices_count[class_] + self.n_samples > len(self.label_to_indices[class_]):\n",
        "                    np.random.shuffle(self.label_to_indices[class_])\n",
        "                    self.used_label_indices_count[class_] = 0\n",
        "            yield indices\n",
        "            self.count += self.n_classes * self.n_samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_dataset // self.batch_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFDXOItgeh9f"
      },
      "source": [
        "Then we create dataloader with the help of `torch.utils.data.DataLoader`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxTGVoIYeh9f"
      },
      "outputs": [],
      "source": [
        "train_batch_sampler = BalancedBatchSampler(train_dataset.targets, n_classes=10, n_samples=25)\n",
        "test_batch_sampler = BalancedBatchSampler(test_dataset.targets, n_classes=10, n_samples=25)\n",
        "\n",
        "triplets_train_loader = torch.utils.data.DataLoader(train_dataset, batch_sampler=train_batch_sampler)\n",
        "triplets_test_loader = torch.utils.data.DataLoader(test_dataset, batch_sampler=test_batch_sampler)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZR3H0Eqeh9g"
      },
      "source": [
        "## Random Triplet Selector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axXrwB8Ieh9g"
      },
      "source": [
        "For the images in a minibatch, we exhaust all the anchor-positive pairs, but random choose one negative example for each positive pair to create triplets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x20ijdOteh9g"
      },
      "outputs": [],
      "source": [
        "from itertools import combinations\n",
        "\n",
        "class RandomTripletSelector():\n",
        "    \"\"\"\n",
        "    Select random negative  example for  each positive pair  to create triplets\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(RandomTripletSelector, self).__init__()\n",
        "\n",
        "    def get_triplets(self, embeddings, labels):\n",
        "        labels = labels.cpu().data.numpy()\n",
        "        triplets = []\n",
        "        for label in set(labels):\n",
        "            label_mask = (labels == label)\n",
        "            label_indices = np.where(label_mask)[0]\n",
        "            if len(label_indices) < 2:\n",
        "                continue\n",
        "            negative_indices = np.where(np.logical_not(label_mask))[0]\n",
        "            anchor_positives = list(combinations(label_indices, 2))  # All anchor-positive pairs\n",
        "\n",
        "            # random choose one negative example for each positive pair\n",
        "            temp_triplets = [[anchor_positive[0], anchor_positive[1], np.random.choice(negative_indices)] for anchor_positive in anchor_positives]\n",
        "            triplets += temp_triplets\n",
        "\n",
        "        return torch.LongTensor(np.array(triplets))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TA8Le96Peh9g"
      },
      "source": [
        "## Triplet Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0y_d0Mq3eh9g"
      },
      "source": [
        "Recall the triplets funtion:\n",
        "\n",
        "**Triplet loss**:   $L_{triplet}(x_a, x_p, x_n) = max(0, m +  \\lVert f(x_a)-f(x_p)\\rVert_2^2 - \\lVert f(x_a)-f(x_n)\\rVert_2^2$\\)\n",
        "\n",
        "Let's write the loss accordingly.\n",
        "\n",
        "Note that, triplets are generated using triplet_selector object that takes embeddings and targets and returns indices of triplets\n",
        "\n",
        "**Hint**:  Please reference notebook chapter 4.5.2 `One-shot/metric learning` for more detail"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cq7GGcz_eh9i"
      },
      "outputs": [],
      "source": [
        "class TripletLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Triplets loss\n",
        "    Takes a batch of embeddings and corresponding labels.\n",
        "    Triplets are generated using triplet_selector object that take embeddings and targets and return indices of\n",
        "    triplets\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, margin, triplet_selector):\n",
        "        super(TripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "        self.triplet_selector = triplet_selector\n",
        "\n",
        "    def forward(self, embeddings, target):\n",
        "\n",
        "        triplets = self.triplet_selector.get_triplets(embeddings, target)\n",
        "\n",
        "        if embeddings.is_cuda:\n",
        "            triplets = triplets.cuda()\n",
        "\n",
        "\n",
        "        anchor_idx= triplets[:, 0]\n",
        "        positive_idx= triplets[:, 1]\n",
        "        negative_idx= triplets[:, 2]\n",
        "\n",
        "\n",
        "        ap_distances = (embeddings[anchor_idx] - embeddings[positive_idx]).pow(2).sum(1)  # .pow(.5)\n",
        "        an_distances = (embeddings[anchor_idx] - embeddings[negative_idx]).pow(2).sum(1)  # .pow(.5)\n",
        "        losses = F.relu(ap_distances - an_distances + self.margin)\n",
        "\n",
        "        return losses.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrU0mIUOeh9i"
      },
      "source": [
        "## Training the  model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HriPT5xVeh9i"
      },
      "source": [
        "Next let's train the model. First, we recall the `Trainer` class from tutorial P2.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kejGt5Ueh9i"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "class Trainer():\n",
        "    def __init__(self,\n",
        "                 model: torch.nn.Module,\n",
        "                 device: torch.device,\n",
        "                 criterion: torch.nn.Module,\n",
        "                 optimizer: torch.optim.Optimizer,\n",
        "                 training_DataLoader: torch.utils.data.Dataset,\n",
        "                 validation_DataLoader: torch.utils.data.Dataset ,\n",
        "                 epochs: int\n",
        "                 ):\n",
        "\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.training_DataLoader = training_DataLoader\n",
        "        self.validation_DataLoader = validation_DataLoader\n",
        "        self.device = device\n",
        "        self.epochs = epochs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def run_trainer(self):\n",
        "\n",
        "\n",
        "        for epoch in tqdm(range(self.epochs)):\n",
        "\n",
        "\n",
        "\n",
        "            self.model.train()  # train mode\n",
        "\n",
        "            train_losses=[]\n",
        "            for batch in self.training_DataLoader:\n",
        "\n",
        "                x,y=batch\n",
        "                input, target = x.to(self.device), y.to(self.device)  # send to device (GPU or CPU)\n",
        "                self.optimizer.zero_grad()  # zerograd the parameters\n",
        "                out = self.model(input)  # one forward pass\n",
        "                loss = self.criterion(out, target)  # calculate loss\n",
        "\n",
        "                loss_value = loss.item()\n",
        "                train_losses.append(loss_value)\n",
        "\n",
        "                loss.backward()  # one backward pass\n",
        "                self.optimizer.step()  # update the parameters\n",
        "\n",
        "\n",
        "\n",
        "            self.model.eval()  # evaluation mode\n",
        "            valid_losses = []  # accumulate the losses here\n",
        "\n",
        "            for batch in self.validation_DataLoader:\n",
        "\n",
        "                x,y=batch\n",
        "                input, target = x.to(self.device), y.to(self.device)  # send to device (GPU or CPU)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    out = self.model(input)   # one forward pass\n",
        "                    loss = self.criterion(out, target) # calculate loss\n",
        "\n",
        "                    loss_value = loss.item()\n",
        "                    valid_losses.append(loss_value)\n",
        "\n",
        "\n",
        "\n",
        "            # print the results\n",
        "            print(\n",
        "                f'EPOCH: {epoch+1:0>{len(str(self.epochs))}}/{self.epochs}',\n",
        "                end=' '\n",
        "            )\n",
        "            print(f'LOSS: {np.mean(train_losses):.4f}',end=' ')\n",
        "            print(f'VAL-LOSS: {np.mean(valid_losses):.4f}',end='\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAXnniYNeh9i"
      },
      "source": [
        "Let's train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2X9BP7IYeh9i"
      },
      "outputs": [],
      "source": [
        "# device\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device=torch.device('cpu')\n",
        "\n",
        "# model\n",
        "embedding_net = EmbeddingNet()\n",
        "model = embedding_net.to(device)\n",
        "\n",
        "\n",
        "# margin value\n",
        "margin=1\n",
        "\n",
        "# criterion\n",
        "criterion = TripletLoss(margin,  RandomTripletSelector())\n",
        "\n",
        "# optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# trainer\n",
        "trainer = Trainer(model=model,\n",
        "                  device=device,\n",
        "                  criterion=criterion,\n",
        "                  optimizer=optimizer,\n",
        "                  training_DataLoader=triplets_train_loader,\n",
        "                  validation_DataLoader=triplets_test_loader,\n",
        "                  epochs=10)\n",
        "\n",
        "# start training\n",
        "trainer.run_trainer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2J7LJBSeh9j"
      },
      "source": [
        "Let's write a help funtion to extract the embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmiHi7sneh9j"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=256)\n",
        "\n",
        "def extract_embeddings(dataloader, model):\n",
        "\n",
        "    cuda = torch.cuda.is_available()\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        embeddings = np.zeros((len(dataloader.dataset), 10))\n",
        "        labels = np.zeros(len(dataloader.dataset))\n",
        "        k = 0\n",
        "        for images, target in dataloader:\n",
        "            if cuda:\n",
        "                images = images.cuda()\n",
        "            embeddings[k:k+len(images)] = model.get_embedding(images).data.cpu().numpy()\n",
        "            labels[k:k+len(images)] = target.numpy()\n",
        "            k += len(images)\n",
        "    return embeddings, labels\n",
        "\n",
        "train_embeddings, train_labels = extract_embeddings(train_loader, model)\n",
        "val_embeddings, val_labels = extract_embeddings(test_loader, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1mQ-mNfeh9k"
      },
      "source": [
        "Then we can visualise the embedding by dimension reduction using `Tsne`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCBm_hCEeh9k"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "def plot_tsne_embeddings(embeddings, targets, xlim=None, ylim=None):\n",
        "\n",
        "\n",
        "    # The first 3000 embeddings and targets\n",
        "    embeddings= embeddings[:3000]\n",
        "    targets= targets[:3000]\n",
        "\n",
        "    # Using Tsne to for dimension reduction\n",
        "    tsne = TSNE(n_components=2)\n",
        "    embeddings = tsne.fit_transform(embeddings)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(10,10))\n",
        "    for i in range(10):\n",
        "        inds = np.where(targets==i)[0]\n",
        "        plt.scatter(embeddings[inds,0], embeddings[inds,1], alpha=0.5)\n",
        "    if xlim:\n",
        "        plt.xlim(xlim[0], xlim[1])\n",
        "    if ylim:\n",
        "        plt.ylim(ylim[0], ylim[1])\n",
        "    plt.legend(classes)\n",
        "\n",
        "plot_tsne_embeddings(train_embeddings, train_labels)\n",
        "plot_tsne_embeddings(val_embeddings, val_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m20GQmn8eh9k"
      },
      "source": [
        "## Refactor the TripletSelector to Select Informative Triplets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tAGaadDeh9l"
      },
      "source": [
        "Based on the definition of the loss, there are three categories of triplets:\n",
        "\n",
        "\n",
        "  *  easy triplets: triplets which have a loss of 0, because `d(a,p)+margin<d(a,n)`\n",
        "  *  hard triplets: triplets where the negative is closer to the anchor than the positive, i.e. `d(a,n)<d(a,p)`\n",
        "  *  semi-hard triplets: triplets where the negative is not closer to the anchor than the positive, but which still have positive loss ` d(a,p)<d(a,n)<d(a,p)+margin`\n",
        "\n",
        "\n",
        "Each of these definitions depend on where the negative is, relatively to the anchor and positive. We can therefore extend these three categories to the negatives: hard negatives, semi-hard negatives or easy negatives.\n",
        "\n",
        "The figure below shows the three corresponding regions of the embedding space for the negative:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IW_jhrs0eh9l"
      },
      "source": [
        "\n",
        "<center><img src=\"https://raw.githubusercontent.com/vlamen/tue-deeplearning/main/img/triplets.png\" alt=\"triplets\" width=\"500\"/></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUzeNtcBeh9l"
      },
      "source": [
        "Our goal is to mining `hard triplets` and `semi-hard triplets`, i.e. `loss_values > 0`, because they are both informative.  To be specific, for the images in a minibatch, we exhaust all the anchor-positive pairs, for each positive pair we randomly choose one negative example in the `hard negatives` or `semi-hard negatives` to create `hard triplet` or `semi-hard triplet`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OE1Va8VIeh9l"
      },
      "source": [
        "First, let's create a help function to get the distances matrix of embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwgUvLwZeh9m"
      },
      "outputs": [],
      "source": [
        "\n",
        "def pdist(vectors):\n",
        "    distance_matrix = -2 * vectors.mm(torch.t(vectors)) + vectors.pow(2).sum(dim=1).view(1, -1) + vectors.pow(2).sum(\n",
        "        dim=1).view(-1, 1)\n",
        "    return distance_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9xGH8pieh9n"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Informative_Negative_TripletSelector():\n",
        "\n",
        "    def __init__(self, margin):\n",
        "        super(Informative_Negative_TripletSelector, self).__init__()\n",
        "\n",
        "        self.margin = margin\n",
        "\n",
        "   # Our goal is to mining informative triplets.\n",
        "    def informative_negative(self, loss_values):\n",
        "\n",
        "        informative_negative = np.where(loss_values > 0)[0]\n",
        "        return np.random.choice(informative_negative) if len(informative_negative) > 0 else None\n",
        "\n",
        "\n",
        "    def get_triplets(self, embeddings, labels):\n",
        "\n",
        "        if torch.cuda.is_available()==False:\n",
        "            embeddings = embeddings.cpu()\n",
        "        distance_matrix = pdist(embeddings)\n",
        "        distance_matrix = distance_matrix.cpu()\n",
        "\n",
        "        labels = labels.cpu().data.numpy()\n",
        "        triplets = []\n",
        "\n",
        "        for label in set(labels):\n",
        "            label_mask = (labels == label)\n",
        "            label_indices = np.where(label_mask)[0]\n",
        "            if len(label_indices) < 2:\n",
        "                continue\n",
        "            negative_indices = np.where(np.logical_not(label_mask))[0]\n",
        "            anchor_positives = list(combinations(label_indices, 2))  # All anchor-positive pairs\n",
        "            anchor_positives = np.array(anchor_positives)\n",
        "\n",
        "\n",
        "            ap_distances = distance_matrix[anchor_positives[:, 0], anchor_positives[:, 1]]\n",
        "            for anchor_positive, ap_distance in zip(anchor_positives, ap_distances):\n",
        "                loss_values = ap_distance - distance_matrix[torch.LongTensor(np.array([anchor_positive[0]])), torch.LongTensor(negative_indices)] + self.margin\n",
        "                loss_values = loss_values.data.cpu().numpy()\n",
        "\n",
        "                hard_negative = self.informative_negative(loss_values)\n",
        "                if hard_negative is not None:\n",
        "                    hard_negative = negative_indices[hard_negative]\n",
        "                    triplets.append([anchor_positive[0], anchor_positive[1], hard_negative])\n",
        "\n",
        "        if len(triplets) == 0:\n",
        "            triplets.append([anchor_positive[0], anchor_positive[1], negative_indices[0]])\n",
        "\n",
        "        triplets = np.array(triplets)\n",
        "\n",
        "        return torch.LongTensor(triplets)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrdO3vdpeh9n"
      },
      "source": [
        "Then we train the model with the mined triplets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEWFwKCneh9n",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# device\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device=torch.device('cpu')\n",
        "\n",
        "# model\n",
        "mined_embedding_net = EmbeddingNet()\n",
        "mined_model = mined_embedding_net.to(device)\n",
        "\n",
        "\n",
        "# margin value\n",
        "margin=1\n",
        "\n",
        "# criterion\n",
        "criterion = TripletLoss(margin,  Informative_Negative_TripletSelector(margin))\n",
        "\n",
        "# optimizer\n",
        "optimizer = torch.optim.SGD(mined_model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# trainer\n",
        "trainer = Trainer(model=mined_model,\n",
        "                  device=device,\n",
        "                  criterion=criterion,\n",
        "                  optimizer=optimizer,\n",
        "                  training_DataLoader=triplets_train_loader,\n",
        "                  validation_DataLoader=triplets_test_loader,\n",
        "                  epochs=10)\n",
        "\n",
        "# start training\n",
        "trainer.run_trainer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQ3dxhVIeh9o"
      },
      "source": [
        "And  visualise the embedding by dimension reduction using Tsne"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwboZtbVeh9o"
      },
      "outputs": [],
      "source": [
        "m_train_embeddings, m_train_labels = extract_embeddings(train_loader, mined_model)\n",
        "m_val_embeddings, m_val_labels = extract_embeddings(test_loader, mined_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBdqtz0beh9p"
      },
      "outputs": [],
      "source": [
        "plot_tsne_embeddings(m_train_embeddings, m_train_labels)\n",
        "plot_tsne_embeddings(m_val_embeddings, m_val_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7k3OT6Jeh9p"
      },
      "source": [
        "# Image Retrieval\n",
        "\n",
        "We are now using the trained model for image retrieval on the `test images`.\n",
        "We are considering, in turn, single images from `test images` as query image, and the remaining 9,999 images as retrieval database.\n",
        "The task of image retrieval (IR) is to find the *K* most similar images to the query image by Euclidean distance.\n",
        "\n",
        "\n",
        "In purpose of this practical is:\n",
        "\n",
        "1.  Perform image retrieval for the first *10* images from `test images`. Retrieve the *K=5* most similar images for each query (first *10* image). Show the query image and the retrieved images next to each other, and print their labels\n",
        "\n",
        "2.   Compute and report the *mean average precision* (mAP), by computing  the *average precision* (AP) for each image in `test images`, and taking the mean AP over all 10,000 images.\n",
        "\n",
        "3.  Compare the results of random triplets selection and  informative triplets selection\n",
        "\n",
        "Hints:\n",
        "\n",
        "\n",
        "*   The AP is defined as follows.\n",
        "  *   Let *TP* be the number of *true positives*, that is, the number of retrieved images which have the *same* label as the query image.\n",
        "  *   Let *FP* be the number of *false positives*, that is, the number of retrieved images which have a *different* label than the query image.\n",
        "  *   Let *FN* be the number of *false negatives*, that is, the number of *non-retrieved* images, which have the *same* label as the query image.\n",
        "  *   The *precision* of an IR algorithm is defined as *precision* := TP / (TP + FP).   \n",
        "  *   The *recall* is defined as *recall* := TP / (TP + FN).\n",
        "  *   Precision will be high if you carefully select very few objects, where you are sure that these are needles. But recall will be low then. Recall will be high if you just grab and return the whole images. But precision will be low then. Thus, precision and recall are (usually) opposed to each other and represent a trade-off.\n",
        "  * This trade-off can typically be governed by some hyper-parameter, in our case *K*, the number of retrieved images. For large *K*, we have large recall but low precision, for small *K* we have higher precision but low recall.\n",
        "  * The trade-off can be inspected by looking at the precision-recall curve. The AP is defined as area under the precision-recall curve.\n",
        "  *   Fortunately, an estimator of AP is already implemented for you in the function *average_precision*. It takes two arguments:\n",
        "     *  sorted_class_vals: list of **class values** of the 9,999 other images, sorted according to closeness to the query image (closest first, most distant last).\n",
        "     *  true_class: the class values of the query image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mCB_SDkeh9q"
      },
      "outputs": [],
      "source": [
        "def average_precision(sorted_class_vals, true_class):\n",
        "    ind = sorted_class_vals == true_class\n",
        "    num_positive = np.sum(ind)\n",
        "    cum_ind = np.cumsum(ind).astype(np.float32)\n",
        "    enum = np.array(range(1, len(ind)+1)).astype(np.float32)\n",
        "    return np.sum(cum_ind * ind / enum) / num_positive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAWi_ybGeh9q"
      },
      "source": [
        "## Performing Image Retrieval with Random Triplets Selection Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQW2HOLJeh9q"
      },
      "source": [
        "Let us perform image retrieval for the first 10 images from test data. Retrieve the K=5 most similar images for each query."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aphif_kEeh9r"
      },
      "source": [
        "First, lets extract the embedding by funtion `extract_embeddings`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxKMUbRbeh9r"
      },
      "outputs": [],
      "source": [
        "val_embeddings, val_labels = extract_embeddings(test_loader, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuUC5i0Deh9r"
      },
      "source": [
        "We compute the distances matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W53sGVVzeh9r"
      },
      "outputs": [],
      "source": [
        "from scipy.spatial.distance import cdist\n",
        "distances=cdist(val_embeddings,val_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQPuSNIOeh9r"
      },
      "source": [
        "Then we load the test images and test labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3i9bvCMeh9r"
      },
      "outputs": [],
      "source": [
        "test_labels=np.array(test_dataset.targets)\n",
        "test_images=test_dataset.data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weYm13Eweh9r"
      },
      "source": [
        "Plot the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rULwV7MDeh9r"
      },
      "outputs": [],
      "source": [
        "for k in range(10):\n",
        "    sorted_idx = list((np.argsort(distances[k,:])))\n",
        "    sorted_idx = sorted_idx[1:]\n",
        "\n",
        "    plt.subplot(1, 6, 1)\n",
        "    plt.imshow(test_dataset.data[k])\n",
        "    for l in range(5):\n",
        "      plt.subplot(1, 6, 2 + l)\n",
        "      plt.imshow(test_images[sorted_idx[l]])\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    print(\"True class:\")\n",
        "    print(classes[test_labels[k]])\n",
        "    print(\"Top 5 predicted:\")\n",
        "    print([classes[test_dataset.targets[i]] for i in sorted_idx[:5]])\n",
        "    print(\"Average precision {}\".format(average_precision(test_labels[sorted_idx], test_labels[k])))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyyloDH-eh9s"
      },
      "source": [
        "Let's computing and report the *mean average precision* (mAP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwb9ThJueh9s"
      },
      "outputs": [],
      "source": [
        "N_mAP = test_images.shape[0]\n",
        "mAP = 0.0\n",
        "for k in range(N_mAP):\n",
        "    sorted_idx = list((np.argsort(distances[k,:])))\n",
        "    sorted_idx = sorted_idx[1:]\n",
        "    mAP += average_precision(test_labels[sorted_idx], test_labels[k])\n",
        "mAP /= N_mAP\n",
        "\n",
        "print('mAP = {}'.format(mAP))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8k8DaUgPeh9s"
      },
      "source": [
        "##  Comparing with Informative Triplets Selection Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nWweS0aeh9t"
      },
      "outputs": [],
      "source": [
        "from scipy.spatial.distance import cdist\n",
        "distances=cdist(m_val_embeddings,m_val_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQARdH9Xeh9u"
      },
      "outputs": [],
      "source": [
        "for k in range(10):\n",
        "    sorted_idx = list((np.argsort(distances[k,:])))\n",
        "    sorted_idx = sorted_idx[1:]\n",
        "\n",
        "    plt.subplot(1, 6, 1)\n",
        "    plt.imshow(test_dataset.data[k])\n",
        "    for l in range(5):\n",
        "      plt.subplot(1, 6, 2 + l)\n",
        "      plt.imshow(test_images[sorted_idx[l]])\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    print(\"True class:\")\n",
        "    print(classes[test_labels[k]])\n",
        "    print(\"Top 5 predicted:\")\n",
        "    print([classes[test_dataset.targets[i]] for i in sorted_idx[:5]])\n",
        "    print(\"Average precision {}\".format(average_precision(test_labels[sorted_idx], test_labels[k])))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Er3d_7W6eh9u"
      },
      "outputs": [],
      "source": [
        "N_mAP = test_images.shape[0]\n",
        "mAP = 0.0\n",
        "for k in range(N_mAP):\n",
        "    sorted_idx = list((np.argsort(distances[k,:])))\n",
        "    sorted_idx = sorted_idx[1:]\n",
        "    mAP += average_precision(test_labels[sorted_idx], test_labels[k])\n",
        "mAP /= N_mAP\n",
        "\n",
        "print('mAP = {}'.format(mAP))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pY1c4XoJdlNV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "iREC7DTfeh9c"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}